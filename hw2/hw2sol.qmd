---
title: "Biostat 212a Homework 2"
subtitle: "Due Feb 6, 2024 @ 11:59PM"
author: "Chengwu Duan (Jason) and UID: 606332825"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 4.8.1 (10pts)

Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In
other words, the logistic function representation and logit representation
for the logistic regression model are equivalent.

$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}} && (4.2)\\ 
p(X) + p(X) \cdot e^{\beta_0 + \beta_1X} &= e^{\beta_0 + \beta_1X} \\
p(X) &= (1 - p(X)) \cdot e^{\beta_0 + \beta_1X} \\  \frac{p(X)}{1 - p(X)} &= e^{\beta_0 + \beta_1X} && (4.3)\\
\end{align*}
$$
## ISL Exercise 4.8.6 (10pts)

Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\hat{\beta}_0 = −6, \hat{\beta}_1 = 0.05, \hat{\beta}_2 = 1$.

\(a\) Estimate the probability that a student who studies for 40 h and
has an undergrad GPA of 3.5 gets an A in the class.

**Answer: **
$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}\\
&= \frac{e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}\\
&= \frac{e^{0.5}}{1 + e^{0.5}}\\
&= 0.622\\
\end{align*}
$$

\(b\) How many hours would the student in part (a) need to study to
have a 50 % chance of getting an A in the class?

**Answer: **
$$
\begin{align*}
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}\\
0.5 &= \frac{e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}\\
0.5 + 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} - 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
1 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
\ln(1) &= -6 + 0.05 \cdot X + 1 \cdot 3.5\\
6 - 3.5 &= 0.05 \cdot X\\
X &= \frac{6 - 3.5}{0.05}\\
X &= 50\\
\end{align*}
$$

## ISL Exercise 4.8.9 (10pts)

This problem has to do with odds.

\(a\) On average, what fraction of people with an odds of 0.37 of
defaulting on their credit card payment will in fact default?

**Answer: **
\[\begin{align*} & \frac{p(X)}{1 - p(X)} = 0.37 \\ \implies & p(X) = 0.37 - 0.37 \cdot p(X) \\ \implies & p(X) = \frac{0.37}{1.37} \end{align*}\]

$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
\frac{p(X)}{1 - p(X)} &= 0.37 \\
p(X) &= 0.37 - 0.37 \cdot p(X) \\ 
p(X) + 0.37 \cdot p(X)&= 0.37\\
p(X) &= \frac{0.37}{1.37}\\
&= 0.27\\
\end{align*}
$$

\(b\) Suppose that an individual has a 16 % chance of defaulting on
her credit card payment. What are the odds that she will default?

**Answer: **
$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
&= \frac{0.16}{1 - 0.16}\\
&= 0.19\\
\end{align*}
$$

## ISL Exercise 4.8.13 (a)-(i) (50pts)

This question should be answered using the Weekly data set, which
is part of the ISLR2 package. This data is similar in nature to the
Smarket data from this chapter’s lab, except that it contains 1, 089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.

(a) Produce some numerical and graphical summaries of the Weekly
data. Do there appear to be any patterns?

(b) Use the full data set to perform a logistic regression with
Direction as the response and the five lag variables plus Volume
as predictors. Use the summary function to print the results. Do
any of the predictors appear to be statistically significant? If so,
which ones?

(c) Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you
about the types of mistakes made by logistic regression.

(d) Now fit the logistic regression model using a training data period
from 1990 to 2008, with Lag2 as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions
for the held out data (that is, the data from 2009 and 2010).

(e) Repeat (d) using LDA.

(f) Repeat (d) using QDA.

(g) Repeat (d) using KNN with K = 1.

(h) Repeat (d) using naive Bayes.

(i) Which of these methods appears to provide the best results on
this data?


## Bonus question: ISL Exercise 4.8.13 Part (j) (30pts)

(j) Experiment with different combinations of predictors, including
possible transformations and interactions, for each of the
methods. Report the variables, method, and associated confusion
matrix that appears to provide the best results on the held
out data. Note that you should also experiment with values for
K in the KNN classifier.

## Bonus question: ISL Exercise 4.8.4 (30pts)

When the number of features p is large, there tends to be a deterioration
in the performance of KNN and other local approaches that
perform prediction using only observations that are near the test observation
for which a prediction must be made. This phenomenon is
known as the curse of dimensionality, and it ties into the fact that curse of dinon-
parametric approaches often perform poorly when p is large. We mensionality
will now investigate this curse.

(a) Suppose that we have a set of observations, each with measurements
on p = 1 feature, X. We assume that X is uniformly
(evenly) distributed on [0, 1]. Associated with each observation
is a response value. Suppose that we wish to predict a test observation’s
response using only observations that are within 10 % of
the range of X closest to that test observation. For instance, in
order to predict the response for a test observation with X = 0.6,
we will use observations in the range [0.55, 0.65]. On average,
what fraction of the available observations will we use to make
the prediction?

(b) Now suppose that we have a set of observations, each with
measurements on p = 2 features, X1 and X2. We assume that
(X1,X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to
predict a test observation’s response using only observations that
are within 10 % of the range of X1 and within 10 % of the range
of X2 closest to that test observation. For instance, in order to
predict the response for a test observation with X1 = 0.6 and
X2 = 0.35, we will use observations in the range [0.55, 0.65] for
X1 and in the range [0.3, 0.4] for X2. On average, what fraction
of the available observations will we use to make the prediction?

(c) Now suppose that we have a set of observations on p = 100 features.
Again the observations are uniformly distributed on each
feature, and again each feature ranges in value from 0 to 1. We
wish to predict a test observation’s response using observations
within the 10 % of each feature’s range that is closest to that test
observation. What fraction of the available observations will we
use to make the prediction?

(d) Using your answers to parts (a)–(c), argue that a drawback of
KNN when p is large is that there are very few training observations
“near” any given test observation.

(e) Now suppose that we wish to make a prediction for a test observation
by creating a p-dimensional hypercube centered around
the test observation that contains, on average, 10 % of the training
observations. For p = 1, 2, and 100, what is the length of each
side of the hypercube? Comment on your answer.
Note: A hypercube is a generalization of a cube to an arbitrary
number of dimensions. When p = 1, a hypercube is simply a line
segment, when p = 2 it is a square, and when p = 100 it is a
100-dimensional cube.
