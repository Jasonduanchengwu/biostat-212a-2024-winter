---
title: "Biostat 212a Homework 2"
subtitle: "Due Feb 6, 2024 @ 11:59PM"
author: "Chengwu Duan (Jason) and UID: 606332825"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 4.8.1 (10pts)

Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In
other words, the logistic function representation and logit representation
for the logistic regression model are equivalent.

$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}} && (4.2)\\ 
p(X) + p(X) \cdot e^{\beta_0 + \beta_1X} &= e^{\beta_0 + \beta_1X} \\
p(X) &= (1 - p(X)) \cdot e^{\beta_0 + \beta_1X} \\  \frac{p(X)}{1 - p(X)} &= e^{\beta_0 + \beta_1X} && (4.3)\\
\end{align*}
$$
## ISL Exercise 4.8.6 (10pts)

Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\hat{\beta}_0 = −6, \hat{\beta}_1 = 0.05, \hat{\beta}_2 = 1$.

\(a\) Estimate the probability that a student who studies for 40 h and
has an undergrad GPA of 3.5 gets an A in the class.

**Answer: **
$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}\\
&= \frac{e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}\\
&= \frac{e^{0.5}}{1 + e^{0.5}}\\
&= 0.622\\
\end{align*}
$$

\(b\) How many hours would the student in part (a) need to study to
have a 50 % chance of getting an A in the class?

**Answer: **
$$
\begin{align*}
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}\\
0.5 &= \frac{e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}\\
0.5 + 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} - 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
1 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
\ln(1) &= -6 + 0.05 \cdot X + 1 \cdot 3.5\\
6 - 3.5 &= 0.05 \cdot X\\
X &= \frac{6 - 3.5}{0.05}\\
X &= 50\\
\end{align*}
$$

## ISL Exercise 4.8.9 (10pts)

This problem has to do with odds.

\(a\) On average, what fraction of people with an odds of 0.37 of
defaulting on their credit card payment will in fact default?

**Answer: **
\[\begin{align*} & \frac{p(X)}{1 - p(X)} = 0.37 \\ \implies & p(X) = 0.37 - 0.37 \cdot p(X) \\ \implies & p(X) = \frac{0.37}{1.37} \end{align*}\]

$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
\frac{p(X)}{1 - p(X)} &= 0.37 \\
p(X) &= 0.37 - 0.37 \cdot p(X) \\ 
p(X) + 0.37 \cdot p(X)&= 0.37\\
p(X) &= \frac{0.37}{1.37}\\
&= 0.27\\
\end{align*}
$$

\(b\) Suppose that an individual has a 16 % chance of defaulting on
her credit card payment. What are the odds that she will default?

**Answer: **
$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
&= \frac{0.16}{1 - 0.16}\\
&= 0.19\\
\end{align*}
$$

## ISL Exercise 4.8.13 (a)-(i) (50pts)

This question should be answered using the Weekly data set, which
is part of the ISLR2 package. This data is similar in nature to the
Smarket data from this chapter’s lab, except that it contains 1, 089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.

```{r, eval=T}
library(ISLR2)
data(Weekly)

head(Weekly)
```

(a) Produce some numerical and graphical summaries of the Weekly
data. Do there appear to be any patterns?

**Answer: **
```{r, eval=T}
summary(Weekly)
```


(b) Use the full data set to perform a logistic regression with
Direction as the response and the five lag variables plus Volume
as predictors. Use the summary function to print the results. Do
any of the predictors appear to be statistically significant? If so,
which ones?

(c) Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you
about the types of mistakes made by logistic regression.

(d) Now fit the logistic regression model using a training data period
from 1990 to 2008, with Lag2 as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions
for the held out data (that is, the data from 2009 and 2010).

(e) Repeat (d) using LDA.

(f) Repeat (d) using QDA.

(g) Repeat (d) using KNN with K = 1.

(h) Repeat (d) using naive Bayes.

(i) Which of these methods appears to provide the best results on
this data?


## Bonus question: ISL Exercise 4.8.13 Part (j) (30pts)

(j) Experiment with different combinations of predictors, including
possible transformations and interactions, for each of the
methods. Report the variables, method, and associated confusion
matrix that appears to provide the best results on the held
out data. Note that you should also experiment with values for
K in the KNN classifier.

## Bonus question: ISL Exercise 4.8.4 (30pts)

When the number of features p is large, there tends to be a deterioration
in the performance of KNN and other local approaches that
perform prediction using only observations that are near the test observation
for which a prediction must be made. This phenomenon is
known as the curse of dimensionality, and it ties into the fact that
non-parametric approaches often perform poorly when p is large. We
will now investigate this curse.

(a) Suppose that we have a set of observations, each with measurements
on p = 1 feature, X. We assume that X is uniformly
(evenly) distributed on [0, 1]. Associated with each observation
is a response value. Suppose that we wish to predict a test observation’s
response using only observations that are within 10 % of
the range of X closest to that test observation. For instance, in
order to predict the response for a test observation with X = 0.6,
we will use observations in the range [0.55, 0.65]. On average,
what fraction of the available observations will we use to make
the prediction?

**Answer: **

Since X is uniformly distributed on [0,1] \(X \sim \mathcal{U}(0, 1)\), the location of a test observation within this range will also be uniformly distributed. Location of each test observations is independent. Therefore, the fraction of observations used to make each prediction will also be 10% since the given range is 10%.

However there could be ‘edge cases’ where \(X \in [0, 0.05) \cup (0.95, 1]\), I inferred that if \(X \in [0, 0.05)\), training observations in the range \([0, 0.1]\) will be used. On the other extreme if \(X \in (0.95, 1]\), training observations in the range \([0.9, 1]\) will be used
These will cause the range to be lower than 10% because there aren’t any training observations outside of \([0,1]\) that can be used, and as a result lowering the fraction of observations used to make the prediction to less than 10%. This will bring down the average fraction of observations used to make the prediction to less than or about 10%.

So on average, we can claim that the fraction of observations used to make each prediction will be slightly lower or about 10%.

(b) Now suppose that we have a set of observations, each with
measurements on p = 2 features, X1 and X2. We assume that
(X1,X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to
predict a test observation’s response using only observations that
are within 10 % of the range of X1 and within 10 % of the range
of X2 closest to that test observation. For instance, in order to
predict the response for a test observation with X1 = 0.6 and
X2 = 0.35, we will use observations in the range [0.55, 0.65] for
X1 and in the range [0.3, 0.4] for X2. On average, what fraction
of the available observations will we use to make the prediction?

**Answer: **

Since we want the fraction of test observations that satisfy both predictors ranges, assuming that the two predictors are independent, the fraction of observations used to make each prediction will be 10% * 10% = 1%.

However like I mentioned in the previous question, there will be edge cases where the fraction of observations used to make the prediction will be less than 1% because there aren’t any training observations outside of \([0,1]\) that can be used, and as a result lowering the fraction of observations used to make the prediction to less than 1%. This will bring down the average fraction of observations used to make the prediction to less than or about 1%.

On average, we could say that the fraction of observations used to make each prediction will be lower or about 1%.

(c) Now suppose that we have a set of observations on p = 100 features.
Again the observations are uniformly distributed on each
feature, and again each feature ranges in value from 0 to 1. We
wish to predict a test observation’s response using observations
within the 10 % of each feature’s range that is closest to that test
observation. What fraction of the available observations will we
use to make the prediction?

**Answer: **

Assuming that the predictors are independent, the fraction of observations used to make each prediction will be \(0.1^{100}\) which is a very small number. This is because the fraction of observations used to make each prediction will be 10% * 10% * 10% * ... * 10% = \(0.1^{100}\). This is a very small number because the fraction of observations used to make each prediction will be 10% for each of the 100 features. This will bring down the average fraction of observations used to make the prediction to a very small number.

(d) Using your answers to parts (a)–(c), argue that a drawback of
KNN when p is large is that there are very few training observations
“near” any given test observation.

**Answer: **

The fraction of observations that could count as a neighbour will be very small when p is large. This is because the fraction of observations used to make each prediction will be in this case, 10% for each of the features. As the the number of predictor increases, the smaller the fraction of test observations will be available to make prediction. This means that there are very few training observations “near” any given test observation. Making it harder to make accurate predictions.

(e) Now suppose that we wish to make a prediction for a test observation
by creating a p-dimensional hypercube centered around
the test observation that contains, on average, 10 % of the training
observations. For p = 1, 2, and 100, what is the length of each
side of the hypercube? Comment on your answer.
Note: A hypercube is a generalization of a cube to an arbitrary
number of dimensions. When p = 1, a hypercube is simply a line
segment, when p = 2 it is a square, and when p = 100 it is a
100-dimensional cube.

**Answer: **

$$
\begin{align*}
0.1 = length^p\\
length = 0.1^\frac{1}{p}\\
\end{align*}
$$

If we are fixing the fraction of training observations to be 10%, then the length of each side of the hypercube will be \(0.1^\frac{1}{p}\).

When \(p\) = 1, the length will be \(0.1^\frac{1}{1} = 0.1\)

When \(p\) = 2, the length must be \(0.1^\frac{1}{2}\) = 0.316. In this case, the hypercube is a square with side length of 0.316. In order to maintain 10% of the observations we need to extend the length of the sides, since we know that keeping it at length of 0.1 will result in only 1% of the training observations available for prediction.

When \(p\) = 100, the length of the sides must be \(0.1^\frac{1}{p} = 0.1^\frac{1}{100} = 0.977\). In order for 10% of the training observations to fall within the hypercube with all 100 sides, all sides of the cube must extend further.

However we can see that as the number of predictors increases, the length of the sides of the hypercube will increase. This means that the hypercube will cover a larger area of the feature space. This will make it harder to make accurate predictions because the hypercube will cover a larger area of the feature space, and as a result, the fraction of training observations that will be used to make the prediction will be lower than 10%. As we can see for \(p = 100\), the length of the sides of the hypercube is 0.977, which is very close to 1. This means that the hypercube will cover almost the entire feature space, and as a result, the fraction of training observations that will be used to make the prediction will be very low.