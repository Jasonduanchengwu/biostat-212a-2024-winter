---
title: "Biostat 212a Homework 2"
subtitle: "Due Feb 6, 2024 @ 11:59PM"
author: "Chengwu Duan (Jason) and UID: 606332825"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 4.8.1 (10pts)

Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In
other words, the logistic function representation and logit representation
for the logistic regression model are equivalent.

$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}} && (4.2)\\ 
p(X) + p(X) \cdot e^{\beta_0 + \beta_1X} &= e^{\beta_0 + \beta_1X} \\
p(X) &= (1 - p(X)) \cdot e^{\beta_0 + \beta_1X} \\  \frac{p(X)}{1 - p(X)} &= e^{\beta_0 + \beta_1X} && (4.3)\\
\log_{e}(\frac{p(X)}{1 - p(X)}) &= \beta_0 + \beta_1X
\end{align*}
$$
## ISL Exercise 4.8.6 (10pts)

Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\hat{\beta}_0 = −6, \hat{\beta}_1 = 0.05, \hat{\beta}_2 = 1$.

\(a\) Estimate the probability that a student who studies for 40 h and
has an undergrad GPA of 3.5 gets an A in the class.

**Answer: **
$$ 
\begin{align*} 
p(X) &= \frac{e^{\beta_0 + \beta_1X_1+\beta_2X_2}}
{1 + e^{\beta_0 + \beta_1X_1+\beta_2X_2}}\\
&= \frac{e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot 40 + 1 \cdot 3.5}}\\
&= \frac{e^{-0.5}}{1 + e^{-0.5}}\\
&= \frac{0.6065}{1 + 0.6065}\\
&= 0.3775\\
\end{align*}
$$

\(b\) How many hours would the student in part (a) need to study to
have a 50 % chance of getting an A in the class?

**Answer: **
$$
\begin{align*}
p(X) &= \frac{e^{\beta_0 + \beta_1X + \beta_2X_2}}{1 + e^{\beta_0 + \beta_1X + \beta_2X_2}}\\
0.5 &= \frac{e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}{1 + e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}}\\
0.5 + 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5} - 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
0.5 &= 0.5 \cdot e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
1 &= e^{-6 + 0.05 \cdot X + 1 \cdot 3.5}\\
\ln(1) &= -6 + 0.05 \cdot X + 1 \cdot 3.5\\
6 - 3.5 &= 0.05 \cdot X\\
X &= \frac{6 - 3.5}{0.05}\\
X &= 50\\
\end{align*}
$$

## ISL Exercise 4.8.9 (10pts)

This problem has to do with odds.

\(a\) On average, what fraction of people with an odds of 0.37 of
defaulting on their credit card payment will in fact default?

**Answer: **
\[\begin{align*} & \frac{p(X)}{1 - p(X)} = 0.37 \\ \implies & p(X) = 0.37 - 0.37 \cdot p(X) \\ \implies & p(X) = \frac{0.37}{1.37} \end{align*}\]

$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
\frac{p(X)}{1 - p(X)} &= 0.37 \\
p(X) &= 0.37 - 0.37 \cdot p(X) \\ 
p(X) + 0.37 \cdot p(X)&= 0.37\\
p(X) &= \frac{0.37}{1.37}\\
&= 0.27\\
\end{align*}
$$

\(b\) Suppose that an individual has a 16 % chance of defaulting on
her credit card payment. What are the odds that she will default?

**Answer: **
$$
\begin{align*}
odds &= \frac{p(X)}{1 - p(X)}\\
&= \frac{0.16}{1 - 0.16}\\
&= 0.19\\
\end{align*}
$$

## ISL Exercise 4.8.13 (a)-(i) (50pts)

This question should be answered using the Weekly data set, which
is part of the ISLR2 package. This data is similar in nature to the
Smarket data from this chapter’s lab, except that it contains 1, 089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.

```{r, eval=T}
library(ISLR2)
data(Weekly)

head(Weekly)
```
The variables are:

Year: The recorded year of obs
Lag1: % return for previous week
Lag2: % return for 2 weeks previous
Lag3: % return for 3 weeks previous
Lag4: % return for 4 weeks previous
Lag5: % return for 5 weeks previous
Volume: average number of daily shares traded in billions
Today: % return for current week
Direction: A factor with levels Down and Up indicating whether the market had a  negative or positive return on a given week

(a) Produce some numerical and graphical summaries of the Weekly
data. Do there appear to be any patterns?

**Answer: **

```{r, eval=T}
summary(Weekly)
```

```{r, eval=T, message=F}
library(ggplot2)
library(GGally)
ggpairs(Weekly)
```

```{r, eval=T, message=F}}
# excludes the Direction column since it is a response variable
cor(Weekly[ ,-9])
```
Based on the corvariates statistics, there are no apparent strong relationships between the `lag` variables. This means that the lag variables are not correlated with each other. And the only strong correlation is between `Year` and `Volume` which is 0.84. This means that the volume of trades has increased over the year, which we can see clearer in a plot of volume over year.

```{r, eval=T, message=F}
library(ggplot2)
ggplot(Weekly, aes(x = Year, y = Volume)) + geom_point() + geom_smooth()+
  labs(title="Volume over Year")
```

Based on the volume over year, there is a significant increase in volume over the years. The volume trend peeked during 2009 and decreased in 2010. This could be due to the financial crisis that happened in 2008.

```{r, eval=T, message=F}
ggplot(Weekly, aes(x = Year, fill = Direction)) + 
  geom_bar(position = "fill") +
  geom_hline(yintercept = 0.5) +
  labs(y="Percentage", title="Direction vs Years")
```
Based on the Direction vs Time plot, majority of the years have higher than 50% of the weeks with positive returns. While there are some years that have more weeks with negative returns than positive returns, particularly in year 2000, 2001, 2002, and 2008.

(b) Use the full data set to perform a logistic regression with
Direction as the response and the five lag variables plus Volume
as predictors. Use the summary function to print the results. Do
any of the predictors appear to be statistically significant? If so,
which ones?

**Answer: **

```{r, eval=T, message=F}
lm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)

summary(lm.fit)
```
The only significant predictor is `Lag2` since it has p-values less than 0.05. This suggest that there is significant evidence that `Lag2` is associated with the response variable `Direction`.

(c) Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you
about the types of mistakes made by logistic regression.

**Answer: **

```{r, eval=T, message=F}
library(caret)

predicted <- factor(ifelse(predict(lm.fit, type = "response") < 0.5, "Down", "Up"))
confusionMatrix(predicted, Weekly$Direction, positive = "Up")
```
The confusion matrix is telling us that the logistic regression model made 56.1% correct predictions with true positives and true negatives over all predictions. 

A sensitivity of 0.9207 suggests that this model is good at predicting the market going up when it actually went up. A specificity of 0.1116 suggests that this model is not good at predicting the market going down when it actually went down. 

Pos Pred Value and Neg Pred Value indicate the precision of positive predictions and negative predictions respectively. Both values are relatively low, which means that the model is not very precise in predicting the market going up or down.

(d) Now fit the logistic regression model using a training data period
from 1990 to 2008, with Lag2 as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions
for the held out data (that is, the data from 2009 and 2010).

**Answer: **

```{r, eval=T, message=F}
train = Weekly[Weekly$Year <= 2008,]
test = Weekly[Weekly$Year > 2008,]

lm.fit = glm(Direction ~ Lag2, data = train, family = binomial)

predicted <- factor(ifelse(predict(lm.fit, newdata = test, type = "response") < 0.5, "Down", "Up"))

confusionMatrix(predicted, test$Direction, positive = "Up")
```
The accuracy is 0.625 while the baseline classifier is 0.5865 (No Information Rate), we can see that the P-Value [Acc > NIR] = 0.2439, which is more than 0.05 assuming we chose alpha to be 0.05, this means that the model is not significantly better than the baseline classifier.

(e) Repeat (d) using LDA.

**Answer: **

```{r, eval=T, message=F}
library(MASS)
lda.fit = lda(Direction ~ Lag2, data = train)
predicted = predict(lda.fit, newdata = test)
confusionMatrix(predicted$class, test$Direction, positive = "Up")
```
We had similar results as the glm results. The accuracy is 0.625 while the baseline classifier is 0.5865 (No Information Rate), we can see that the P-Value [Acc > NIR] = 0.2439, which is more than 0.05 assuming we chose alpha to be 0.05, this means that the model is not significantly better than the baseline classifier.

(f) Repeat (d) using QDA.

**Answer: **

```{r, eval=T, message=F}
qda.fit = qda(Direction ~ Lag2, data = train)
predicted = predict(qda.fit, newdata = test)
confusionMatrix(predicted$class, test$Direction, positive = "Up")

```
The accuracy is 0.5865 while the baseline classifier is also 0.5865 (No Information Rate). We can see that there is no difference and as the P-Value [Acc > NIR] = 0.5419, which is more than 0.05 assuming we chose alpha to be 0.05, this means that the model is not significantly better than the baseline classifier. The QDA predicts direction for every test observation, which is why the accuracy is the same as the baseline classifier, with a sensitivity of 1 and a specificity of 0.

(g) Repeat (d) using KNN with K = 1.

**Answer: **

```{r, eval=T, message=F}
library(class)
train.X = cbind(train$Lag2)
test.X = cbind(test$Lag2)
train.Direction = train$Direction
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
confusionMatrix(knn.pred, test$Direction, positive = "Up")
```
In this case with KNN with K = 1, the accuracy is 0.5 while the baseline classifier is 0.5865 (No Information Rate). We can see that it is not performing better than the baseline and the P-value gives us the same conclusion.

(h) Repeat (d) using naive Bayes.

**Answer: **

```{r, eval=F, message=F}
library(e1071)
naive.fit = naiveBayes(Direction ~ Lag2, data = train)
predicted = predict(naive.fit, newdata = test)
confusionMatrix(predicted, test$Direction, positive = "Up")
```
The accuracy model is 0.5865 which is same as the baseline in this case. The P-value [Acc > NIR] = 0.5419, which is more than 0.05 assuming we chose alpha to be 0.05, this means that the model is not significantly better than the baseline classifier.

(i) Which of these methods appears to provide the best results on
this data?

**Answer: **

Logistic regression and LDA appear to provide the best results on this data. Both models have the same accuracy of 0.625, which is the highest among all the models.


## Bonus question: ISL Exercise 4.8.13 Part (j) (30pts)

(j) Experiment with different combinations of predictors, including
possible transformations and interactions, for each of the
methods. Report the variables, method, and associated confusion
matrix that appears to provide the best results on the held
out data. Note that you should also experiment with values for
K in the KNN classifier.

## Bonus question: ISL Exercise 4.8.4 (30pts)

When the number of features p is large, there tends to be a deterioration
in the performance of KNN and other local approaches that
perform prediction using only observations that are near the test observation
for which a prediction must be made. This phenomenon is
known as the curse of dimensionality, and it ties into the fact that
non-parametric approaches often perform poorly when p is large. We
will now investigate this curse.

(a) Suppose that we have a set of observations, each with measurements
on p = 1 feature, X. We assume that X is uniformly
(evenly) distributed on [0, 1]. Associated with each observation
is a response value. Suppose that we wish to predict a test observation’s
response using only observations that are within 10 % of
the range of X closest to that test observation. For instance, in
order to predict the response for a test observation with X = 0.6,
we will use observations in the range [0.55, 0.65]. On average,
what fraction of the available observations will we use to make
the prediction?

**Answer: **

Since X is uniformly distributed on [0,1] \(X \sim \mathcal{U}(0, 1)\), the location of a test observation within this range will also be uniformly distributed. Location of each test observations is independent. Therefore, the fraction of observations used to make each prediction will also be 10% since the given range is 10%.

However there could be ‘edge cases’ where \(X \in [0, 0.05) \cup (0.95, 1]\), I inferred that if \(X \in [0, 0.05)\), training observations in the range \([0, 0.1]\) will be used. On the other extreme if \(X \in (0.95, 1]\), training observations in the range \([0.9, 1]\) will be used
These will cause the range to be lower than 10% because there aren’t any training observations outside of \([0,1]\) that can be used, and as a result lowering the fraction of observations used to make the prediction to less than 10%. This will bring down the average fraction of observations used to make the prediction to less than or about 10%.

So on average, we can claim that the fraction of observations used to make each prediction will be slightly lower or about 10%.

(b) Now suppose that we have a set of observations, each with
measurements on p = 2 features, X1 and X2. We assume that
(X1,X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to
predict a test observation’s response using only observations that
are within 10 % of the range of X1 and within 10 % of the range
of X2 closest to that test observation. For instance, in order to
predict the response for a test observation with X1 = 0.6 and
X2 = 0.35, we will use observations in the range [0.55, 0.65] for
X1 and in the range [0.3, 0.4] for X2. On average, what fraction
of the available observations will we use to make the prediction?

**Answer: **

Since we want the fraction of test observations that satisfy both predictors ranges, assuming that the two predictors are independent, the fraction of observations used to make each prediction will be 10% * 10% = 1%.

However like I mentioned in the previous question, there will be edge cases where the fraction of observations used to make the prediction will be less than 1% because there aren’t any training observations outside of \([0,1]\) that can be used, and as a result lowering the fraction of observations used to make the prediction to less than 1%. This will bring down the average fraction of observations used to make the prediction to less than or about 1%.

On average, we could say that the fraction of observations used to make each prediction will be lower or about 1%.

(c) Now suppose that we have a set of observations on p = 100 features.
Again the observations are uniformly distributed on each
feature, and again each feature ranges in value from 0 to 1. We
wish to predict a test observation’s response using observations
within the 10 % of each feature’s range that is closest to that test
observation. What fraction of the available observations will we
use to make the prediction?

**Answer: **

Assuming that the predictors are independent, the fraction of observations used to make each prediction will be \(0.1^{100}\) which is a very small number. This is because the fraction of observations used to make each prediction will be 10% * 10% * 10% * ... * 10% = \(0.1^{100}\). This is a very small number because the fraction of observations used to make each prediction will be 10% for each of the 100 features. This will bring down the average fraction of observations used to make the prediction to a very small number.

(d) Using your answers to parts (a)–(c), argue that a drawback of
KNN when p is large is that there are very few training observations
“near” any given test observation.

**Answer: **

The fraction of observations that could count as a neighbour will be very small when p is large. This is because the fraction of observations used to make each prediction will be in this case, 10% for each of the features. As the the number of predictor increases, the smaller the fraction of test observations will be available to make prediction. This means that there are very few training observations “near” any given test observation. Making it harder to make accurate predictions.

(e) Now suppose that we wish to make a prediction for a test observation
by creating a p-dimensional hypercube centered around
the test observation that contains, on average, 10 % of the training
observations. For p = 1, 2, and 100, what is the length of each
side of the hypercube? Comment on your answer.
Note: A hypercube is a generalization of a cube to an arbitrary
number of dimensions. When p = 1, a hypercube is simply a line
segment, when p = 2 it is a square, and when p = 100 it is a
100-dimensional cube.

**Answer: **

$$
\begin{align*}
0.1 = length^p\\
length = 0.1^\frac{1}{p}\\
\end{align*}
$$

If we are fixing the fraction of training observations to be 10%, then the length of each side of the hypercube will be \(0.1^\frac{1}{p}\).

When \(p\) = 1, the length will be \(0.1^\frac{1}{1} = 0.1\)

When \(p\) = 2, the length must be \(0.1^\frac{1}{2}\) = 0.316. In this case, the hypercube is a square with side length of 0.316. In order to maintain 10% of the observations we need to extend the length of the sides, since we know that keeping it at length of 0.1 will result in only 1% of the training observations available for prediction.

When \(p\) = 100, the length of the sides must be \(0.1^\frac{1}{p} = 0.1^\frac{1}{100} = 0.977\). In order for 10% of the training observations to fall within the hypercube with all 100 sides, all sides of the cube must extend further.

However we can see that as the number of predictors increases, the length of the sides of the hypercube will increase. This means that the hypercube will cover a larger area of the feature space. This will make it harder to make accurate predictions because the hypercube will cover a larger area of the feature space, and as a result, the fraction of training observations that will be used to make the prediction will be lower than 10%. As we can see for \(p = 100\), the length of the sides of the hypercube is 0.977, which is very close to 1. This means that the hypercube will cover almost the entire feature space, and as a result, the fraction of training observations that will be used to make the prediction will be very low.