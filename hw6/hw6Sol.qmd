---
title: "Biostat 212a Homework 6"
subtitle: "Due Mar 22, 2024 @ 11:59PM"
author: "Chengwu Duan and 606332825"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

Load R libraries.
```{r}
rm(list = ls())
library(tidyverse)
library(tidymodels)
library(readr)
library(tswge)
library(ggplot2)
library(yardstick)
library(workflows)
library(parsnip)
library(tidyclust)
library(RcppHungarian)

acfdf <- function(vec) {
    vacf <- acf(vec, plot = F)
    with(vacf, data.frame(lag, acf))
}

ggacf <- function(vec) {
    ac <- acfdf(vec)
    ggplot(data = ac, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + 
        geom_segment(mapping = aes(xend = lag, yend = 0))
}

tplot <- function(vec) {
    df <- data.frame(X = vec, t = seq_along(vec))
    ggplot(data = df, aes(x = t, y = X)) + geom_line()
}
```

## New York Stock Exchange (NYSE) data (1962-1986) (140 pts)

::: {#fig-nyse}

<p align="center">
![](ISL_fig_10_14.pdf){width=600px height=600px}
</p>

Historical trading statistics from the New York Stock Exchange. Daily values of the normalized log trading volume, DJIA return, and log volatility are shown for a 24-year period from 1962-1986. We wish to predict trading volume on any day, given the history on all earlier days. To the left of the red bar (January 2, 1980) is training data, and to the right test data.

:::

The [`NYSE.csv`](https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/NYSE.csv) file contains three daily time series from the New York Stock Exchange (NYSE) for the period Dec 3, 1962-Dec 31, 1986 (6,051 trading days).

- `Log trading volume` ($v_t$): This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.
    
- `Dow Jones return` ($r_t$): This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.
    
- `Log volatility` ($z_t$): This is based on the absolute values of daily price movements.

```{r}
# Read in NYSE data from url

url = "https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/NYSE.csv"
NYSE <- read_csv(url)

NYSE
```
The **autocorrelation** at lag $\ell$ is the correlation of all pairs $(v_t, v_{t-\ell})$ that are $\ell$ trading days apart. These sizable correlations give us confidence that past values will be helpful in predicting the future.

```{r}
#| code-fold: true
#| label: fig-nyse-autocor
#| fig-cap: "The autocorrelation function for log volume. We see that nearby values are fairly strongly correlated, with correlations above 0.2 as far as 20 days apart."

ggacf(NYSE$log_volume) + ggthemes::theme_few()

```

Do a similar plot for (1) the correlation between $v_t$ and lag $\ell$ `Dow Jones return` $r_{t-\ell}$ and (2) correlation between $v_t$ and lag $\ell$ `Log volatility` $z_{t-\ell}$.

```{r}
#| code-fold: true
#| label: fig-v-vs-lagged-r
#| fig-cap: "Correlations between log_volume and lagged DJ_return."

seq(1, 30) %>% 
  map(function(x) {cor(NYSE$log_volume , lag(NYSE$DJ_return, x), use = "pairwise.complete.obs")}) %>% 
  unlist() %>% 
  tibble(lag = 1:30, cor = .) %>% 
  ggplot(aes(x = lag, y = cor)) + 
  geom_hline(aes(yintercept = 0)) + 
  geom_segment(mapping = aes(xend = lag, yend = 0)) + 
  ggtitle("AutoCorrelation between `log volume` and lagged `DJ return`")
```

```{r}
#| code-fold: true
#| label: fig-v-vs-lagged-z
#| fig-cap: "Weak correlations between log_volume and lagged log_volatility."

seq(1, 30) %>% 
  map(function(x) {cor(NYSE$log_volume , lag(NYSE$log_volatility, x), use = "pairwise.complete.obs")}) %>% 
  unlist() %>% 
  tibble(lag = 1:30, cor = .) %>% 
  ggplot(aes(x = lag, y = cor)) + 
  geom_hline(aes(yintercept = 0)) + 
  geom_segment(mapping = aes(xend = lag, yend = 0)) + 
  ggtitle("AutoCorrelation between `log volume` and lagged `log volatility`")
```



### Project goal

Our goal is to forecast daily `Log trading volume`, using various machine learning algorithms we learnt in this class. 

The data set is already split into train (before Jan 1st, 1980, $n_{\text{train}} = 4,281$) and test (after Jan 1st, 1980, $n_{\text{test}} = 1,770$) sets.

<!-- Include `day_of_week` as a predictor in the models. -->

In general, we will tune the lag $L$ to acheive best forecasting performance. In this project, we would fix $L=5$. That is we always use the previous five trading days' data to forecast today's `log trading volume`.

Pay attention to the nuance of splitting time series data for cross validation. Study and use the [`time-series`](https://www.tidymodels.org/learn/models/time-series/) functionality in tidymodels. Make sure to use the same splits when tuning different machine learning algorithms.

Use the $R^2$ between forecast and actual values as the cross validation and test evaluation criterion.

### Baseline method (20 pts)

We use the straw man (use yesterday’s value of `log trading volume` to predict that of today) as the baseline method. Evaluate the $R^2$ of this method on the test data.


Before we tune different machine learning methods, let's first separate into the test and non-test sets. We drop the first 5 trading days which lack some lagged variables.

```{r}
# Lag: look back L trading days
# Do not need to include, as we included them in receipe
L = 5

for(i in seq(1, L)) {
  NYSE <- NYSE %>% 
    mutate(!!paste("DJ_return_lag", i, sep = "") := lag(NYSE$DJ_return, i),
           !!paste("log_volume_lag", i, sep = "") := lag(NYSE$log_volume, i),
           !!paste("log_volatility_lag", i, sep = "") := lag(NYSE$log_volatility, i))
}


NYSE <- NYSE %>% na.omit()
```


```{r}
# Drop beginning trading days which lack some lagged variables
NYSE_other <- NYSE %>% 
  filter(train == 'TRUE') %>%
  select(-train)

dim(NYSE_other)
```

```{r}
NYSE_test = NYSE %>% 
  filter(train == 'FALSE') %>%
  select(-train)

dim(NYSE_test)
```

```{r}
library(yardstick)
# cor(NYSE_test$log_volume, NYSE_test$log_volume_lag1) %>% round(2)
r2_test_strawman =  rsq_vec(NYSE_test$log_volume, lag(NYSE_test$log_volume, 1)) %>% round(2)
print(paste("Straw man test R2: ", r2_test_strawman))
```


### Autoregression (AR) forecaster (30 pts)

- Let
$$
y = \begin{pmatrix} v_{L+1} \\ v_{L+2} \\ v_{L+3} \\ \vdots \\ v_T \end{pmatrix}, \quad M = \begin{pmatrix}
1 & v_L & v_{L-1} & \cdots & v_1 \\
1 & v_{L+1} & v_{L} & \cdots & v_2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & v_{T-1} & v_{T-2} & \cdots & v_{T-L}
\end{pmatrix}.
$$

- Fit an ordinary least squares (OLS) regression of $y$ on $M$, giving
$$
\hat v_t = \hat \beta_0 + \hat \beta_1 v_{t-1} + \hat \beta_2 v_{t-2} + \cdots + \hat \beta_L v_{t-L},
$$
known as an **order-$L$ autoregression** model or **AR($L$)**.

- Before we start the model training, let's talk about time series resampling. We will use the `rolling_origin` function in the `rsample` package to create a time series cross-validation plan.

- When the data have a strong time component, a resampling method should support modeling to estimate seasonal and other temporal trends within the data. A technique that randomly samples values from the training set can disrupt the model’s ability to estimate these patterns.


```{r}
NYSE %>% 
  ggplot(aes(x = date, y = log_volume)) + 
  geom_line() + 
  geom_smooth(method = "lm")
```


```{r}
wrong_split <- initial_split(NYSE)

bind_rows(
  training(wrong_split) %>% mutate(type = "train"),
  testing(wrong_split) %>% mutate(type = "test")
) %>%
  ggplot(aes(x = date, y = log_volume, color = type, group = NA)) +
  geom_line()
```

```{r}
correct_split <- initial_time_split(NYSE %>% arrange(date))

bind_rows(
  training(correct_split) %>% mutate(type = "train"),
  testing(correct_split) %>% mutate(type = "test")
) %>%
  ggplot(aes(x = date, y = log_volume, color = type, group = NA)) +
  geom_line()
```

```{r}
rolling_origin(NYSE_other %>% arrange(date), initial = 30, assess = 7) %>%
  mutate(train_data = map(splits, analysis),
         test_data = map(splits, assessment)) %>% 
  select(-splits) %>% 
  pivot_longer(-id) %>% 
  filter(id %in% c("Slice0001", "Slice0002", "Slice0003")) %>% 
  unnest(value) %>% 
  ggplot(aes(x = date, y = log_volume, color = name, group = NA)) + 
  geom_point() + 
  geom_line() +
  facet_wrap(~id, scales = "fixed")
```

```{r}
sliding_period(NYSE_other %>% arrange(date), 
               date, period = "month", lookback = Inf, assess_stop = 1) %>% 
  mutate(train_data = map(splits, analysis),
         test_data = map(splits, assessment)) %>% 
  select(-splits) %>% 
  pivot_longer(-id) %>% 
  filter(id %in% c("Slice001", "Slice002", "Slice003")) %>% 
  unnest(value) %>% 
  ggplot(aes(x = date, y = log_volume, color = name, group = NA)) + 
  geom_point() +
  geom_line() + 
  facet_wrap(~id, scales = "fixed")
```

- Rolling forecast origin resampling ([Hyndman and Athanasopoulos 2018](https://otexts.com/fpp3/)) provides a method that emulates how time series data is often partitioned in practice, estimating the model with historical data and evaluating it with the most recent data. 


- Tune AR(5) with elastic net (enet + ridge) regularization using all 3 features on the training data, and evaluate the test performance. 

### Preprocessing

```{r}
recipe <- 
  recipe(log_volume ~ DJ_return_lag1 + DJ_return_lag2 + DJ_return_lag3 + 
                      DJ_return_lag4 + DJ_return_lag5 + log_volume_lag1 + 
                      log_volume_lag2 + log_volume_lag3 + log_volume_lag4 + 
                      log_volume_lag5 + log_volatility_lag1 + 
                      log_volatility_lag2 + log_volatility_lag3 + 
                      log_volatility_lag4 + log_volatility_lag5, 
         data = NYSE_other) %>% 
  step_dummy(all_nominal()) %>% 
  step_normalize(all_numeric_predictors()) %>%  
  step_naomit(all_predictors())
  # prep(data = NYSE_other)
```

### Model training

```{r, eval=FALSE}
library(doParallel)

### Model
set.seed(203)

enet_mod <- 
  # mixture = 0 (ridge), mixture = 1 (lasso)
  # mixture = (0, 1) elastic net 
  linear_reg(penalty = tune(), 
             mixture = 0.99) %>% 
  set_engine("glmnet")
enet_mod

en_wf <- 
  workflow() %>%
  add_model(enet_mod) %>%
  add_recipe(recipe)
en_wf

write_rds(en_wf, "en_wf.rds")

set.seed(203)
folds <- NYSE_other %>% arrange(date) %>%
    sliding_period(date, period = "month", lookback = Inf, assess_stop = 1)

month_folds <- NYSE_other %>%
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)

lambda_grid <-
  grid_regular(penalty(range = c(-15, 15), 
                       trans = log10_trans()),
               levels = 30)
lambda_grid

# Register doParallel as the parallel backend as Random Forest is computationally expensive
# saving 2 cpu for other tasks
registerDoParallel(cores = detectCores()-2)

en_fit <- tune_grid(en_wf, 
                    resamples = month_folds, 
                    grid = lambda_grid, 
                    metrics = metric_set(rmse, rsq))

# stop the registered parallel backend
stopImplicitCluster()

en_fit

write_rds(en_fit, "en_fit.rds")
```

```{r}
en_fit <- read_rds("en_fit.rds")
en_wf <- read_rds("en_wf.rds")

show_best(en_fit, "rsq")

best_enet <- en_fit %>%
  select_best("rsq")
best_enet

en_fit %>% collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Penalty", y = "R Square") +
  scale_x_log10(labels = scales::label_number())

# Final enet workflow
final_enet_wf <- en_wf %>%
  finalize_workflow(best_enet)
final_enet_wf

final_enet_fit <- 
  final_enet_wf %>%
  last_fit(correct_split)
final_enet_fit %>% collect_metrics()
```

- Hint: [Workflow: enet](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_enet.html) is a good starting point.

**Answer: ** I changed the mixture parameter to 0.99 because in the process of tunning from 0 to 1 by an increment of 0.1 each time, I have discovered that the best performing mixture is closer to 1 (lasso). I also increased the range of the penalty term to (-15, 15) and used 30 levels to capture the maximum R Square. The elastic net model is decent at predicting the log volume with the test rsq being 0.4037 since the baseline model has a test rsq of 0.35 with an 0.0537 improvement. An rsq of 0.4037 means that approximately 40.37% of the variance in the outcome is predictable from the predictors in the model.

### Random forest forecaster (30pts)

- Use the same features as in AR($L$) for the random forest. Tune the random forest and evaluate the test performance.

- Hint: [Workflow: Random Forest for Prediction](https://ucla-biostat-212a.github.io/2024winter/slides/08-tree/workflow_rf_reg.html) is a good starting point.

### Preprocessing

**Answer: ** Same recipe as before since same dataset is used.

### Model training

```{r, eval = FALSE}
library(doParallel)
library(ranger)

set.seed(203)
### Model
rf_mod <- 
  rand_forest(
    mode = "regression",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(recipe)
  # add_recipe(recipe %>% step_rm(date) %>% step_indicate_na())
rf_wf

write_rds(rf_wf, "rf_wf.rds")

set.seed(203)
folds <- NYSE_other %>% arrange(date) %>%
    sliding_period(date, period = "month", lookback = Inf, assess_stop = 1)
  # rolling_origin(initial = 5, assess = 1)


month_folds <- NYSE_other %>%
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)

rf_grid <-
  grid_regular(
    trees(range = c(500L, 600L)), 
    mtry(range = c(3L, 5L)),
    levels = c(2, 2))
rf_grid

# Register doParallel as the parallel backend as Random Forest is computationally expensive
# saving 2 cpu for other tasks
registerDoParallel(cores = detectCores()-2)

rf_fit <- tune_grid(rf_wf, resamples = month_folds, 
                    grid = rf_grid, metrics = metric_set(rmse, rsq)) 

# stop the registered parallel backend
stopImplicitCluster()

write_rds(rf_fit, "rf_fit.rds")
```

```{r}
rf_fit <- read_rds("rf_fit.rds")
rf_wf <- read_rds("rf_wf.rds")

show_best(rf_fit, "rsq")

best_rf <- rf_fit %>%
  select_best("rsq")
best_rf
  
rf_fit %>%
    collect_metrics() %>%
    filter(.metric == "rsq") %>%
    ggplot(mapping = aes(x = trees, 
                         y = mean, 
                         color = mtry)) + 
      geom_point() + 
      geom_line() + 
      labs(x = "trees", y = "R Square") + 
      scale_x_log10(labels = scales::label_number())
rf_fit

# Final enet workflow
final_rf_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_rf_wf

final_rf_fit <- 
  final_rf_wf %>%
  last_fit(correct_split)
final_rf_fit %>% collect_metrics()
```
**Answer: ** Due to limited computing resources, I tuned with small ranges of mtry and trees to locate the optimal tunning
parameters to capture the maximum R Square. The random forest is decent at predicting the log volume with the test rsq being 0.3830 since the baseline model has a test rsq of 0.35, however it is performing worse than elastic net model. An rsq of 0.3830 means that approximately 38.30% of the variance in the outcome is predictable from the predictors in the model.

### Boosting forecaster (30pts)

- Use the same features as in AR($L$) for the boosting. Tune the boosting algorithm and evaluate the test performance.

- Hint: [Workflow: Boosting tree for Prediction](https://ucla-biostat-212a.github.io/2024winter/slides/08-tree/workflow_boosting_reg.html) is a good starting point.

### Preprocessing

**Answer: ** Same recipe as before since same dataset is used.

### Model training

```{r, eval = FALSE}
library(doParallel)
library(xgboost)

gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 500, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod

gb_wf <- 
  workflow() %>%
  add_model(gb_mod) %>%
  add_recipe(recipe)
  # add_recipe(recipe %>% step_rm(date) %>% step_indicate_na())
gb_wf

write_rds(gb_wf, "gb_wf.rds")

folds <- NYSE_other %>% arrange(date) %>%
    sliding_period(date, period = "month", lookback = Inf, assess_stop = 1)
  # rolling_origin(initial = 5, assess = 1)


month_folds <- NYSE_other %>%
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4)

gb_grid <-
  grid_regular(
      tree_depth(range = c(1L, 3L)),
      learn_rate(range = c(-3, 1), trans = log10_trans()),
      levels = c(3, 4))
gb_grid

# Register doParallel as the parallel backend
# saving 2 cpu for other tasks
registerDoParallel(cores = detectCores()-2)

gb_fit <- tune_grid(gb_wf, resamples = month_folds, grid = gb_grid, metrics = metric_set(rmse, rsq)) 

# stop the registered parallel backend
stopImplicitCluster()

write_rds(gb_fit, "gb_fit.rds")
```

```{r}
gb_fit <- read_rds("gb_fit.rds")
gb_wf <- read_rds("gb_wf.rds")

show_best(gb_fit, "rsq")

best_gb <- gb_fit %>%
  select_best("rsq")
best_gb
  
gb_fit %>%
    collect_metrics() %>%
    filter(.metric == "rsq") %>%
    ggplot(mapping = aes(x = learn_rate, 
                         y = mean, 
                         color = tree_depth)) + 
      geom_point() + 
      geom_line() + 
      labs(x = "trees", y = "R Square") + 
      scale_x_log10(labels = scales::label_number())
gb_fit

# Final boosting workflow
final_gb_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_gb_wf

final_gb_fit <- 
  final_gb_wf %>%
  last_fit(correct_split)
final_gb_fit %>% collect_metrics()
```

**Answer: ** Due to limited computing resources, I tuned with small ranges of tree_depth and learn_rate to locate the optimal tunning parameters to capture the maximum R Square. The boosting method is decent at predicting the log volume with the test rsq being 0.4050 since the baseline model has a test rsq of 0.35. An rsq of 0.4050 means that approximately 40.50% of the variance in the outcome is predictable from the predictors in the model.

### Summary (30pts)

Your score for this question is largely determined by your final test performance.

Summarize the performance of different machine learning forecasters in the following format. 

| Method | CV $R^2$ | Test $R^2$ |
|:------:|:------:|:------:|
| Baseline        | NA    | 0.35 |
| AR(5)           | 0.2503 | 0.4037 |
| Random Forest   | 0.2504 | 0.3830 |
| Boosting        | 0.2572 | 0.4050 |

**Answer: ** The best performing model is the Boosting model with a test R2 of 0.4050. The AR(5) model is the second best with a test R2 of 0.4037. The random forest model is the worst performing model with a test R2 of 0.3830. The baseline model has a test R2 of 0.35. There are evidently improvement between the baseline model and the rest of the more elaborate models with about 5% improvement.

Noteably, all models have a lower CV R2 than the test R2. This could be a sign of underfitting possbily. Due to my limited computing resources I am not able to fully explore the hyperparameter space for the optimal models given the time constraint. It could also be due to the outliers or the noise in the training data.

### Extension reading   
- [MOIRAI: Salesforce’s Foundation Model for Time-Series Forecasting](https://towardsdatascience.com/moirai-salesforces-foundation-model-for-time-series-forecasting-4eff6c34093d)

## ISL Exercise 12.6.13 (90 pts)

On the book website, www.statlearning.com, there is a gene expres-
sion data set (Ch12Ex13.csv) that consists of 40 tissue samples with
measurements on 1,000 genes. The first 20 samples are from healthy
patients, while the second 20 are from a diseased group.

(a) Load in the data using read.csv(). You will need to select
header = F.

```{r}
library(tidyverse)
# Read in the gene expression data from url
url = "https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Ch12Ex13.csv"
gene_exp <- read.csv(url, header = FALSE)
gene_exp

# gene_exp_dist = as.dist(1-cor(gene_exp))
```

### 12.6.13 (b) (30 pts)
(b) Apply hierarchical clustering to the samples using correlation-
based distance, and plot the dendrogram. Do the genes separate
the samples into the two groups? Do your results depend on the
type of linkage used?

#### single linkage

```{r}
library(tidyclust)

hc_spec <- hier_clust(
  num_clusters = 2,
  linkage_method = "single"
)

hc_spec

hc_fit <- hc_spec %>%
  fit(~ .,
    data = as.data.frame(t(gene_exp))
  )

hc_fit %>%
  summary()

hc_fit$fit %>% plot()

hc_summary <- hc_fit %>% extract_fit_summary()

hc_summary %>% str()

hc_fit %>% extract_centroids()

hc_preds <- hc_fit %>% predict(as.data.frame(t(gene_exp)))

hc_preds

bind_cols(
  hc_preds,
  extract_cluster_assignment(hc_fit)
)
```

#### Average linkage

```{r}
library(tidyclust)

hc_spec <- hier_clust(
  num_clusters = 2,
  linkage_method = "average"
)

hc_spec

hc_fit <- hc_spec %>%
  fit(~ .,
    data = as.data.frame(t(gene_exp))
  )

hc_fit %>%
  summary()

hc_fit$fit %>% plot()

hc_summary <- hc_fit %>% extract_fit_summary()

hc_summary %>% str()

hc_fit %>% extract_centroids()

hc_preds <- hc_fit %>% predict(as.data.frame(t(gene_exp)))

hc_preds

bind_cols(
  hc_preds,
  extract_cluster_assignment(hc_fit)
)
```

#### complete linkage

```{r}
library(tidyclust)

hc_spec <- hier_clust(
  num_clusters = 2,
  linkage_method = "complete"
)

hc_spec

hc_fit <- hc_spec %>%
  fit(~ .,
    data = as.data.frame(t(gene_exp))
  )

hc_fit %>%
  summary()

hc_fit$fit %>% plot()

hc_summary <- hc_fit %>% extract_fit_summary()

hc_summary %>% str()

hc_fit %>% extract_centroids()

hc_preds <- hc_fit %>% predict(as.data.frame(t(gene_exp)))

hc_preds

bind_cols(
  hc_preds,
  extract_cluster_assignment(hc_fit)
)
```

**Answer: ** The genes separated the samples into the two groups as shown in the dendrograms. Similar results were produced by the three linkage methods.

### PCA and UMAP (30 pts)

```{r}
library(tidymodels)
pca_rec <- recipe(~., data = gene_exp) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())
pca_prep <- prep(pca_rec)
pca_prep

library(tidytext)
tidied_pca <- tidy(pca_prep, 2)


tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  )
```

```{r}
pca_results <- juice(pca_prep)

set.seed(203) # For reproducibility
clusters <- kmeans(pca_results[, c("PC1", "PC2")], centers = 2)

#add the cluster assignments
pca_results$Cluster <- as.factor(clusters$cluster)

#plot the clusters
ggplot(pca_results, aes(PC1, PC2, color = Cluster)) + 
  geom_point(alpha = 0.7, size = 2) +
  labs(color = "Cluster")
```

```{r}
library(embed)
umap_rec <- recipe(~., data = gene_exp) %>%
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors())
umap_prep <- prep(umap_rec)
umap_prep

umap_results <- juice(umap_prep)

set.seed(203) # For reproducibility
clusters <- kmeans(umap_results[, c("UMAP1", "UMAP2")], centers = 2)

#add the cluster assignments
umap_results$Cluster <- as.factor(clusters$cluster)

#plot the clusters
ggplot(umap_results, aes(UMAP1, UMAP2, color = Cluster)) + 
  geom_point(alpha = 0.7, size = 2) +
  labs(color = "Cluster")
```

## TODO: comment on PCA and UMAP results

### 12.6.13 (c) (30 pts)
(c) Your collaborator wants to know which genes differ the most
across the two groups. Suggest a way to answer this question,
and apply it here.

```{r}
library(tidyverse)

url = "https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Ch12Ex13.csv"
Ch12Ex13 <- read_csv(url, col_names = paste("ID", 1:40, sep = ""))
```

```{r}
library(tidyclust)

set.seed(838383)
hc_spec <- hier_clust(
  # num_clusters = 3,
  linkage_method = "average"
)


hc_fit <- hc_spec %>%
  fit(~ .,
    data = as.data.frame(t(Ch12Ex13)) 
  )

hc_fit %>%
  summary()
```

```{r}
hc_fit$fit %>% plot()
```


```{r}
grp = factor(rep(c(1, 0), each = 20))

regression <- function(y) {
  sum <- summary(lm(y ~ grp))
  pv <- sum$coefficients[2, 4]
  return(pv)
}


out <- tibble(gene = seq(1, nrow(Ch12Ex13)),
              p_values = unlist(purrr:: map(1:nrow(Ch12Ex13), ~regression(as.matrix(Ch12Ex13)[.x, ]))))
```

```{r}
out %>% arrange(p_values) %>% head(10)

# sig <- out %>% arrange(p_values) %>% filter(p_values < 0.05/nrow(Ch12Ex13))
sig <- out %>% arrange(p_values) %>% filter(p_values < 0.05 )
```

```{r}
# install.packages("pheatmap")
library(pheatmap)
# install.packages("ggplotify")
library(ggplotify) ## to convert pheatmap to ggplot2
# install.packages("heatmaply")
library(heatmaply) ## for constructing interactive heatmap
```

```{r}

#create data frame for annotations
dfh <- data.frame(sample=as.character(colnames(Ch12Ex13)), status = "disease") %>%
                column_to_rownames("sample")
dfh$status[seq(21, 40)] <-  "healthy"
dfh


pheatmap(Ch12Ex13[sig$gene, ], cluster_rows = FALSE, cluster_cols = T, scale="row", annotation_col = dfh,
         annotation_colors=list(status = c(disease = "orange", healthy = "black")),
         color=colorRampPalette(c("navy", "white", "red"))(50))
```

**Answer: ** To identify the genes that differ the most across the two groups, I performed a linear regression analysis on each gene to determine the p-value of the association between the gene and the group. The heatmap shows that the expression of these genes are different across the two groups.

## TODO: elaborate more


