---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 11, 2024 @ 11:59PM"
author: "Chengwu Duan (Jason) and 606332825"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 9.7.1 (10pts)

This problem involves hyperplanes in two dimensions.

(a) Sketch the hyperplane $1 + 3X_1 − X_2 = 0$. Indicate the set of
points for which $1 + 3X_1 − X_2 > 0$, as well as the set of points
for which $1 + 3X_1 − X_2 < 0$.

```{r}
library(ggplot2)

# Define the range for X1 and X2
x1_range <- seq(-10, 10, length.out = 300)
x2_range <- seq(-30, 30, length.out = 300)

# Create a grid of X1, X2 values
grid <- expand.grid(X1 = x1_range, X2 = x2_range)

# Calculate 1 + 3X1 - X2
grid$hyperplane_value <- with(grid, 1 + 3 * X1 - X2)

# Base plot with points colored based on the hyperplane value
ggplot(grid, aes(x = X1, y = X2, fill = hyperplane_value)) +
  geom_tile() + # Use geom_tile to color areas based on hyperplane_value
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-10, 10), space = "Lab", 
                       name="Hyperplane\nValue") +
  geom_contour(aes(z = hyperplane_value), binwidth = 1, colour = "black") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Hyperplane 1 + 3X1 - X2 = 0",
       x = expression(X[1]),
       y = expression(X[2])) +
  theme_minimal() 
```

**Answer: **
The area above the line, colored in shades of blue, represents the set of points for which $1 + 3X_1 - X_2 < 0$ . This inequality holds true for points on the plane that are below the hyperplane line because the actual value of $X_2$ for these points is greater than $1 + 3X_1$. 

The area below the line, colored in shades of red, represents the set of points for which $1 + 3X_1 - X_2 > 0$. This inequality holds true for points on the plane that are above the hyperplane line because the actual value of $X_2$ for these points is less than $1 + 3X_1$.

The color intensity reflects the magnitude of the value $1 + 3X_1 + X_2$: darker shades indicate values farther from zero (either positive or negative), while lighter shades are closer to zero. The hyperplane itself is represented by the region where the color transitions from blue to red, indicating the value is exactly zero at the line. Points are classified based on the region where it lies in being above the line are classified as positive, while points below the line are classified as negative.

(b) On the same plot, sketch the hyperplane $−2 + X_1 + 2X_2 = 0$.
Indicate the set of points for which $−2 + X_1 + 2X_2 > 0$, as well
as the set of points for which $−2 + X_1 + 2X_2 < 0$.

#### On the same plot

```{r}
library(ggplot2)

# Define the range for X1 and X2
x1_range <- seq(-10, 10, length.out = 300)
x2_range <- seq(-30, 30, length.out = 300)

# Create a grid of X1, X2 values
grid <- expand.grid(X1 = x1_range, X2 = x2_range)

# Calculate 1 + 3X1 - X2
grid$hyperplane_value <- with(grid, -2 + X1 + 2 * X2)

# Base plot with points colored based on the hyperplane value
ggplot(grid, aes(x = X1, y = X2, fill = hyperplane_value)) +
  geom_tile() + # Use geom_tile to color areas based on hyperplane_value
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-10, 10), space = "Lab", 
                       name="Hyperplane\nValue") +
  geom_contour(aes(z = hyperplane_value), binwidth = 1, colour = "black") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Hyperplane -2 + X1 + 2X2 = 0",
       x = expression(X[1]),
       y = expression(X[2])) +
  theme_minimal() 
```

**Answer: ** same logic as part a.

## ISL Exercise 9.7.2 (10pts)

We have seen that in p = 2 dimensions, a linear decision boundary
takes the form $β_0 +β_1X_1 +β_2X_2 = 0$. We now investigate a non-linear
decision boundary.

(a) Sketch the curve
$(1 + X_1)^2 + (2 − X_2)^2 = 4$.

```{r}
library(ggplot2)

# Function to calculate the circle's radius squared
radius_squared <- function(x, y) {
  (1 + x)^2 + (2 - y)^2
}

# Define the range for X1 and X2 and create a data frame
x_range <- seq(-4, 2, length.out = 300)
y_range <- seq(0, 4, length.out = 300)
df <- expand.grid(X1 = x_range, X2 = y_range)

# Calculate the Z values (the radius squared for each point)
df$Z <- mapply(radius_squared, df$X1, df$X2)

# Plot the contour
ggplot(df, aes(X1, X2, z = Z)) +
  stat_contour(breaks = 4, colour = "blue") +
  scale_x_continuous(name = "X1") +
  scale_y_continuous(name = "X2") +
  labs(title = "Contour of (1 + X1)^2 + (2 - X2)^2 = 4") +
  theme_minimal()
```

(b) On your sketch, indicate the set of points for which
$(1 + X_1)^2 + (2 − X_2)^2 > 4$,

as well as the set of points for which $(1 + X_1)^2 + (2 − X_2)^2 ≤ 4$.

**Answer:  **

```{r}
library(ggplot2)

# Function to calculate the circle's radius squared
radius_squared <- function(x, y) {
  (1 + x)^2 + (2 - y)^2
}

# Define the new range for X1 and X2 and create a data frame
x_range <- seq(-6, 4, length.out = 300) # Extended range for X1
y_range <- seq(-2, 6, length.out = 300) # Extended range for X2
df <- expand.grid(X1 = x_range, X2 = y_range)

# Calculate the Z values (the radius squared for each point)
df$Z <- mapply(radius_squared, df$X1, df$X2)

# Plot the contour with the updated ranges
ggplot(df, aes(X1, X2, z = Z)) +
  stat_contour(breaks = 4, colour = "blue") +
  scale_x_continuous(name = "X1") +
  scale_y_continuous(name = "X2") +
  labs(title = "Contour of (1 + X1)^2 + (2 - X2)^2 = 4") +
  annotate("text", x = -1, y = 2, label = "Region where (1 + X1)^2 + (2 - X2)^2 <= 4", color = "red") +
  annotate("text", x = -1.9, y = -0.5, label = "Region where (1 + X1)^2 + (2 - X2)^2 > 4", color = "blue") + 
  theme_minimal()
```

(c) Suppose that a classifier assigns an observation to the blue class
if
$(1 + X_1)^2 + (2 − X_2)^2 > 4$,
and to the red class otherwise. To what class is the observation
(0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?

**Answer: **

The observation (0, 0) is classified as blue because $(1 + 0)^2 + (2 − 0)^2 = 5 > 4$.

The observation (-1, 1) is classified as red because $(1 + (-1))^2 + (2 − 1)^2 = 1 ≤ 4$.

The observation (2, 2) is classified as blue because $(1 + 2)^2 + (2 − 2)^2 = 9 > 4$.

The observation (3, 8) is classified as blue because $(1 + 3)^2 + (2 − 8)^2 = 52 > 4$.

(d) Argue that while the decision boundary in (c) is not linear in
terms of $X_1 and X_2$, it is linear in terms of $X_1, X_1^2 , X_2, and X_2^2$. 

**Answer: **

The decision boundary in (c) is defined by the equation $(1 + X_1)^2 + (2 − X_2)^2 = 4$. This equation can be expanded to the following form:

$$
1 + 2X_1 + X_1^2 + 4 - 4X_2 + X_2^2 = 4
$$
This equation becomes a linear combination of the terms $X_1, X_1^2 , X_2, and X_2^2$. Therefore, the decision boundary is linear in terms of these variables.

## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

### SVM with Radical Kernel

```{r}
# Load libraries
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# For reproducibility
set.seed(203)

# creating an outcome variable
carseats <- Carseats %>%
  mutate(SalesHL = ifelse(Sales <= 8, "Low", "High")) %>%
  select(-Sales)

# Numerical summaries stratified by the outcome `SalesHL`.
carseats %>% tbl_summary(by = SalesHL)

data_split <- initial_split(
  carseats, 
  strata = "SalesHL", 
  prop = 0.75
  )
data_split

carseats_train <- training(data_split)
dim(carseats_train)

carseats_test <- testing(data_split)
dim(carseats_test)

svm_recipe <- 
  recipe(
    SalesHL ~ ., 
    data = carseats_train
  ) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors())
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE)
svm_recipe

svm_mod <- 
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod

svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf

param_grid <- grid_regular(
  cost(range = c(1, 16)),
  rbf_sigma(range = c(-4, -2)),
  levels = c(15, 5)
  )
param_grid

set.seed(203)

folds <- vfold_cv(carseats_train, v = 5)
folds

svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit

svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = cost, y = mean, alpha = rbf_sigma)) +
  geom_point() +
  geom_line(aes(group = rbf_sigma)) +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()

svm_fit %>%
  show_best("roc_auc")

best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm

# Final workflow
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```

### SVM with Polynomial Kernel

```{r}
# Load libraries
library(GGally)
library(gtsummary)
library(kernlab)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# For reproducibility
set.seed(203)

# creating an outcome variable
carseats <- Carseats %>%
  mutate(SalesHL = ifelse(Sales <= 8, "Low", "High")) %>%
  select(-Sales)

# Numerical summaries stratified by the outcome `SalesHL`.
carseats %>% tbl_summary(by = SalesHL)

data_split <- initial_split(
  carseats, 
  strata = "SalesHL", 
  prop = 0.75
  )
data_split

carseats_train <- training(data_split)
dim(carseats_train)

carseats_test <- testing(data_split)
dim(carseats_test)

svm_recipe <- 
  recipe(
    SalesHL ~ ., 
    data = carseats_train
  ) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors())
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE)
svm_recipe

svm_mod <- 
  svm_poly(
    mode = "classification",
    cost = tune(),
    degree = tune(),
  ) %>% 
  set_engine("kernlab")
svm_mod

svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```

```{r}
param_grid <- grid_regular(
  cost(range = c(-3, 2)),
  degree(range = c(1, 5)),
  #scale_factor(range = c(-1, 1)),
  levels = c(5)
  )
param_grid

set.seed(203)

folds <- vfold_cv(carseats_train, v = 5)
folds

svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit

svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = degree, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()
```

```{r}
svm_fit %>%
  show_best("roc_auc")

best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm

# Final workflow
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```

```{r}
library(doParallel)
set.seed(101)
split_obj <- initial_split(data = carseats, prop = 0.7, strata = SalesHL)
train <- training(split_obj)
test <- testing(split_obj)


# Create the recipe
recipe(SalesHL ~ ., data = train) %>%
  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep() -> recipe_obj

# Bake
train <- bake(recipe_obj, new_data=train)
test <- bake(recipe_obj, new_data=test)

library(vip)
final_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  # vip(method = "permute", train= Heart)
  vip(method = "permute", 
      target = "SalesHL", metric = "accuracy",
      pred_wrapper = kernlab::predict, train = train)

svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_rbf_fit <- svm_rbf_spec %>%
  fit(SalesHL ~ ., data = train[, c('Price', 'Age', 'SalesHL')])

svm_rbf_fit %>%
  extract_fit_engine() %>%
  plot()
```

### SVM with Linear Kernel TBD

```{r}

```

#### Word explaination TBD

## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 

**Answer: **

The signed distance of a point $x$ to the hyperplane $f(X) = 0$ is given by the formula:

$$
\frac{f(x)}{\| \beta \|} = \frac{\beta_0 + \beta^T x}{\| \beta \|}
$$

where $\| \beta \|$ is the Euclidean norm of the vector $\beta$. This formula can be derived from the definition of the dot product and the Euclidean norm. The numerator $f(x)$ is the projection of the vector $x$ onto the vector $\beta$, and the denominator $\| \beta \|$ is the length of the vector $\beta$. Therefore, the signed distance of a point $x$ to the hyperplane $f(X) = 0$ is proportional to $f(x)$. 


$$
f(x_0) =  \beta_0 + \beta^T x_0 = 0
$$
Now consider any point x in $\mathbb{R}^p$. Let d be the signed distance from x to the hyperplane along the direction of the normal vector $\beta$. The point on the hyperplane that is closest to x, denoted as $x_0$, can be written in terms of x and d:

$$
x_0 = x - d \frac{\beta}{\| \beta \|} \\
f(x_0) =  \beta_0 + \beta^T x_0 = 0 \\
0 = \beta_0 + \beta^T (x - d \frac{\beta}{\| \beta \|}) \\
0 = \beta_0 + \beta^T x - d \frac{\beta^T \beta}{\| \beta \|} \\
0 = \beta_0 + \beta^T x - d \|\beta\|  \\
\beta_0 + \beta^T x = d \|\beta\|  \\
f(x) = d \|\beta\|  \\
$$

Therefore $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$.