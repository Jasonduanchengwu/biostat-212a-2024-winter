---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Chengwu Duan and 606332825"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 8.4.3 (10pts)

Consider the Gini index, classification error, and entropy in a simple
classification setting with two classes. Create a single plot that displays
each of these quantities as a function of $\hat{p}_{m1}$. The x-axis should
display $\hat{p}_{m1}$, ranging from 0 to 1, and the y-axis should display the
value of the Gini index, classification error, and entropy.

Hint: In a setting with two classes, $\hat{p}_{m1}$ = 1âˆ’ $\hat{p}_{m2}$. You could make
this plot by hand, but it will be much easier to make in R.

**Answer: **

$\hat{p}_{mk}$ is the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. 

The Gini index is defined by

$$
\begin{align}
G &= \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})\\
&= \hat{p}_{m1} (1 - \hat{p}_{m1}) + \hat{p}_{m2} (1 - \hat{p}_{m2})\\
\end{align}
$$
Classification error is defined by

$$
\begin{align}
E &= 1 - \max_k \hat{p}_{mk}\\
&= 1 - \max(\hat{p}_{m1}, \hat{p}_{m2})
\end{align}
$$
Entropy is defined by

$$
\begin{align}
D &= -\sum_{k=1}^{K} \hat{p}_{mk} \log \hat{p}_{mk}\\
&= -\hat{p}_{m1} \log \hat{p}_{m1} - \hat{p}_{m2} \log \hat{p}_{m2}
\end{align}
$$

```{r, message = F, warning=F}
library(tidyverse)
library(ggplot2)

phat.m1 <- seq(0, 1, 0.001)
phat.m2 <- 1 - phat.m1

gini_index <- phat.m1*(1-phat.m1) + phat.m2*(1-phat.m2)
class_error <- 1 - pmax(phat.m1, phat.m2)
entropy <- -phat.m1*log(phat.m1) - phat.m2*log(phat.m2)


data.frame(phat.m1, phat.m2, class_error, gini_index, entropy) %>%
  pivot_longer(cols = c(class_error, gini_index, entropy), names_to = "measures") %>%
    ggplot(aes(x = phat.m1, y = value, col = factor(measures))) + 
    geom_line() + 
    scale_y_continuous(breaks = seq(0, 1, 0.1), minor_breaks = NULL) + 
    scale_color_hue(labels = c("Classification Error", "Entropy", "Gini Index")) +
    labs(title = "Gini Index, Classification Error, and Entropy", 
         subtitle = "as a function of p_hat_m1",
         col = "Measures", 
         y = "Value", 
         x = "Proportion of m1") +
  theme_bw()
```
## ISL Exercise 8.4.4 (10pts)

This question relates to the plots in Figure 8.14.

![](./8.3.png)

(a) Sketch the tree corresponding to the partition of the predictor
space illustrated in the left-hand panel of Figure 8.14. The numbers
inside the boxes indicate the mean of Y within each region.

**Answer: ** 

![](./8.4.4.a.jpg)

(b) Create a diagram similar to the left-hand panel of Figure 8.14,
using the tree illustrated in the right-hand panel of the same
figure. You should divide up the predictor space into the correct
regions, and indicate the mean for each region.

**Answer: ** 

```{r}
library(ggplot2)

# Create an empty plot
ggplot() +
  # set the x and y limits
  xlim(c(-2, 2)) + 
  ylim(c(-3, 3)) +
  # Add rectangles for the regions
  geom_rect(aes(xmin=-2, xmax=2, ymin=2, ymax=3), fill="white", color="black") +
  geom_rect(aes(xmin=0, xmax=2, ymin=1, ymax=2), fill="white", color="black") +
  geom_rect(aes(xmin=-2, xmax=0, ymin=1, ymax=2), fill="white", color="black") +
  geom_rect(aes(xmin=-2, xmax=1, ymin=-3, ymax=1), fill="white", color="black") +
  geom_rect(aes(xmin=1, xmax=2, ymin=-3, ymax=1), fill="white", color="black") +
  # Annotate the regions with the corresponding mean values
  annotate("text", x = 0, y = 2.5, label = "Mean = 2.49", size=4) + 
  annotate("text", x = -1, y = 1.5, label = "Mean = -1.06", size=4) +
  annotate("text", x = 1, y = 1.5, label = "Mean = 0.21", size=4) +
  annotate("text", x = -0.5, y = -1, label = "Mean = -1.80", size=4) +
  annotate("text", x = 1.5, y = -1, label = "Mean = 0.63", size=4) +
  # Add labels and theme adjustments
  labs(x="X1", y="X2", title="Feature Space Divided by Decision Tree") +
  theme_minimal()
```

## ISL Exercise 8.4.5 (10pts)

Suppose we produce ten bootstrapped samples from a data set
containing red and green classes. We then apply a classification tree
to each bootstrapped sample and, for a specific value of X, produce
10 estimates of P(Class is Red|X):

0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.

There are two common ways to combine these results together into a
single class prediction. One is the majority vote approach discussed in
this chapter. The second approach is to classify based on the average
probability. In this example, what is the final classification under each
of these two approaches?

**Answer: **

```{r}
probs <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)

# Majority vote approach
(Majority_vote_approach <- ifelse(sum(probs > 0.5) > 5, "red", "green"))

# Average approach
(Average_approach <- ifelse(mean(probs) > 0.5, "red", "green"))
```

The majority vote approach would classify the observation as red, since 6 out of the 10 trees predict a probability of red more than 0.5. The average probability approach would classify the observation as green, since the average of the 10 probabilities is 0.45.

## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.

**Answer: **

```{r}
sessionInfo()
```
### RF for prediction

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# Numerical summaries stratified by the outcome `AHD`.
Boston %>% tbl_summary()

# reproducibility
set.seed(123)

# Split the data into training and testing sets
data_split <- initial_split(
  Boston, 
  prop = 0.5
  )
data_split

boston_train <- training(data_split)
dim(boston_train)

boston_test <- testing(data_split)
dim(boston_test)

rf_recipe <-
  recipe(
    medv ~ .,
    data = boston_train
  ) %>%
  # create traditional dummy variables (not necessary for random forest in R)
  # omit all NAs in medv
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>%
  # center and scale numeric data (not necessary for random forest)
  # estimate the means and standard deviations
  prep(training = boston_train, retain = TRUE)

rf_recipe

rf_model <-
  rand_forest(
    mode = "regression",
    # number of predictors randomly sampled in each split
    mtry = tune(),
    # number of trees in ensemble
    trees = tune()
  ) %>%
  set_engine("ranger")

rf_model

rf_workflow <-
  workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model)

rf_workflow
```
### TBF - RF grid ranges need adjustments

```{r}
rf_grid <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(1L, 5L)),
  levels = c(3,5)
  )
rf_grid

set.seed(203)

folds <- vfold_cv(boston_train, v = 5)
folds

rf_fit <- rf_workflow %>%
  tune_grid(
    resamples = folds,
    grid = rf_grid,
    metrics = metric_set(rmse, rsq)
    )
rf_fit

rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV mse")

rf_fit %>%
  show_best("rmse")

best_rf <- rf_fit %>%
  select_best("rmse")
best_rf

# Final workflow
final_wf <- rf_workflow %>%
  finalize_workflow(best_rf)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```

### Boosting for prediction

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# Numerical summaries stratified by the outcome `AHD`.
Boston %>% tbl_summary()

# Split the data into training and testing sets
data_split <- initial_split(
  Boston, 
  prop = 0.5
  )
data_split

boston_train <- training(data_split)
dim(boston_train)

boston_test <- testing(data_split)
dim(boston_test)

gb_recipe <- 
  recipe(
    medv ~ ., 
    data = boston_train
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = boston_train, retain = TRUE)
gb_recipe

gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod

gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

### TBF - Boosting grid ranges need adjustments

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
  )
param_grid

set.seed(203)

folds <- vfold_cv(boston_train, v = 5)
folds

gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
gb_fit

gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()

gb_fit %>%
  show_best("rmse")

best_gb <- gb_fit %>%
  select_best("rmse")
best_gb

# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```
```{r}
library(vip)

#library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.

**Answer: **

### RF for classification

```{r}
# Load libraries
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# checking for missing values
summary(Carseats)
colSums(is.na(Carseats))

Carseats %>% tbl_summary()

# For reproducibility
set.seed(203)

# creating an outcome variable
carseats <- Carseats %>%
  mutate(SalesHL = ifelse(Sales <= 8, "Low", "High"))

data_split <- initial_split(
  carseats, 
  # stratify by AHD
  strata = "SalesHL", 
  prop = 0.75
  )
data_split

carseats_train <- training(data_split)
dim(carseats_train)

carseats_test <- testing(data_split)
dim(carseats_test)

rf_recipe <- 
  recipe(
    SalesHL ~ ., 
    data = carseats_train
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  
  # no NAs in carseats
  # # mean imputation for Ca
  # step_impute_mean(Ca) %>%
  # # mode imputation for Thal
  # step_impute_mode(Thal) %>%
  
  
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = carseats_train, retain = TRUE)
rf_recipe

rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf
```

### TBF - RF grid ranges need adjustments

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid

set.seed(203)

folds <- vfold_cv(carseats_train, v = 5)
folds

rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
rf_fit

rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")

rf_fit %>%
  show_best("roc_auc")

best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf

# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```
### boosting for classification

```{r}
# Load libraries
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(xgboost)

# checking for missing values
summary(Carseats)
colSums(is.na(Carseats))

Carseats %>% tbl_summary()

# For reproducibility
set.seed(212)

# creating an outcome variable
carseats <- Carseats %>%
  mutate(SalesHL = ifelse(Sales <= 8, "Low", "High"))

data_split <- initial_split(
  carseats, 
  # stratify by AHD
  strata = "SalesHL", 
  prop = 0.75
  )
data_split

carseats_train <- training(data_split)
dim(carseats_train)

carseats_test <- testing(data_split)
dim(carseats_test)

gb_recipe <- 
  recipe(
    SalesHL ~ ., 
    data = carseats_train
  ) %>%
  # create traditional dummy variables (necessary for xgboost)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # estimate the means and standard deviations
  prep(training = carseats_train, retain = TRUE)
gb_recipe

gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod

gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

### TBF - Boosting grid ranges need adjustments

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid

set.seed(203)

folds <- vfold_cv(carseats_train, v = 5)
folds

gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
gb_fit

gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()

gb_fit %>%
  show_best("roc_auc")

best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb

# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf

# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()
```
