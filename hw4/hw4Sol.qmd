---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Chengwu Duan and 606332825"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 8.4.3 (10pts)

Consider the Gini index, classification error, and entropy in a simple
classification setting with two classes. Create a single plot that displays
each of these quantities as a function of \(\hat{p}_{m1}\). The x-axis should
display \(\hat{p}_{m1}\), ranging from 0 to 1, and the y-axis should display the
value of the Gini index, classification error, and entropy.

Hint: In a setting with two classes, \(\hat{p}_{m1}\) = 1âˆ’ \(\hat{p}_{m2}\). You could make
this plot by hand, but it will be much easier to make in R.

**Answer: **

\(\hat{p}_{mk}\) is the proportion of training observations in the \(m^{th}\) region that are from the \(k^{th}\) class. 

The Gini index is defined by

$$
\begin{align}
G &= \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})\\
&= \hat{p}_{m1} (1 - \hat{p}_{m1}) + \hat{p}_{m2} (1 - \hat{p}_{m2})\\
\end{align}
$$
Classification error is defined by

$$
\begin{align}
E &= 1 - \max_k \hat{p}_{mk}\\
&= 1 - \max(\hat{p}_{m1}, \hat{p}_{m2})
\end{align}
$$
Entropy is defined by

$$
\begin{align}
D &= -\sum_{k=1}^{K} \hat{p}_{mk} \log \hat{p}_{mk}\\
&= -\hat{p}_{m1} \log \hat{p}_{m1} - \hat{p}_{m2} \log \hat{p}_{m2}
\end{align}
$$

```{r, message = F, warning=F}
library(tidyverse)
library(ggplot2)

phat.m1 <- seq(0, 1, 0.001)
phat.m2 <- 1 - phat.m1

gini_index <- phat.m1*(1-phat.m1) + phat.m2*(1-phat.m2)
class_error <- 1 - pmax(phat.m1, phat.m2)
entropy <- -phat.m1*log(phat.m1) - phat.m2*log(phat.m2)


data.frame(phat.m1, phat.m2, class_error, gini_index, entropy) %>%
  pivot_longer(cols = c(class_error, gini_index, entropy), names_to = "measures") %>%
    ggplot(aes(x = phat.m1, y = value, col = factor(measures))) + 
    geom_line() + 
    scale_y_continuous(breaks = seq(0, 1, 0.1), minor_breaks = NULL) + 
    scale_color_hue(labels = c("Classification Error", "Entropy", "Gini Index")) +
    labs(title = "Gini Index, Classification Error, and Entropy", 
         subtitle = "as a function of p_hat_m1",
         col = "Measures", 
         y = "Value", 
         x = "Proportion of m1") +
  theme_bw()
```
## ISL Exercise 8.4.4 (10pts)

This question relates to the plots in Figure 8.14.

![](./8.3.png)

(a) Sketch the tree corresponding to the partition of the predictor
space illustrated in the left-hand panel of Figure 8.14. The numbers
inside the boxes indicate the mean of Y within each region.

**Answer: ** 

![](./8.4.4.a.jpg)

(b) Create a diagram similar to the left-hand panel of Figure 8.14,
using the tree illustrated in the right-hand panel of the same
figure. You should divide up the predictor space into the correct
regions, and indicate the mean for each region.

**Answer: ** 

```{r}
library(ggplot2)

# Create an empty plot
ggplot() +
  # set the x and y limits
  xlim(c(-2, 2)) + 
  ylim(c(-3, 3)) +
  # Add rectangles for the regions
  geom_rect(aes(xmin=-2, xmax=2, ymin=2, ymax=3), fill="white", color="black") +
  geom_rect(aes(xmin=0, xmax=2, ymin=1, ymax=2), fill="white", color="black") +
  geom_rect(aes(xmin=-2, xmax=0, ymin=1, ymax=2), fill="white", color="black") +
  geom_rect(aes(xmin=-2, xmax=1, ymin=-3, ymax=1), fill="white", color="black") +
  geom_rect(aes(xmin=1, xmax=2, ymin=-3, ymax=1), fill="white", color="black") +
  # Annotate the regions with the corresponding mean values
  annotate("text", x = 0, y = 2.5, label = "Mean = 2.49", size=4) + 
  annotate("text", x = -1, y = 1.5, label = "Mean = -1.06", size=4) +
  annotate("text", x = 1, y = 1.5, label = "Mean = 0.21", size=4) +
  annotate("text", x = -0.5, y = -1, label = "Mean = -1.80", size=4) +
  annotate("text", x = 1.5, y = -1, label = "Mean = 0.63", size=4) +
  # Add labels and theme adjustments
  labs(x="X1", y="X2", title="Feature Space Divided by Decision Tree") +
  theme_minimal()
```

## ISL Exercise 8.4.5 (10pts)

Suppose we produce ten bootstrapped samples from a data set
containing red and green classes. We then apply a classification tree
to each bootstrapped sample and, for a specific value of X, produce
10 estimates of P(Class is Red|X):

0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.

There are two common ways to combine these results together into a
single class prediction. One is the majority vote approach discussed in
this chapter. The second approach is to classify based on the average
probability. In this example, what is the final classification under each
of these two approaches?

**Answer: **

```{r}
probs <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)

# Majority vote approach
(Majority_vote_approach <- ifelse(sum(probs > 0.5) > 5, "red", "green"))

# Average approach
(Average_approach <- ifelse(mean(probs) > 0.5, "red", "green"))
```

The majority vote approach would classify the observation as red, since 6 out of the 10 trees predict a probability of red more than 0.5. The average probability approach would classify the observation as green, since the average of the 10 probabilities is 0.45.

## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.

**Answer: **

## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.

**Answer: **

