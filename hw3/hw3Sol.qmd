---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "YOUR NAME and UID"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)

We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.

(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.

**Answer: ** 

Assume that there are n total observations, there is an equal probability of selecting any of the n observations. The probability of selecting the $j^{th}$ observation is 1/n. Thus the probability of not selecting the $j^{th}$ observation is 1 - 1/n.

(b) What is the probability that the second bootstrap observation
is not the jth observation from the original sample?

**Answer: **

Same as in part (a), the probability of not selecting the $j^{th}$ observation is 1 - 1/n.

(c) Argue that the probability that the jth observation is not in the
bootstrap sample is (1 − 1/n)n.

**Answer: **

since we sample n times in total and each sampling is independent, the probability of not selecting the $j^{th}$ observation is $(1 - 1/n)^n$.

(d) When n = 5, what is the probability that the jth observation is
in the bootstrap sample?

**Answer: **

When n = 5, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/5)^5 = 0.672$ according to previous established equation.

(e) When n = 100, what is the probability that the jth observation
is in the bootstrap sample?

**Answer: **

When n = 100, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/100)^100 = 0.634$ according to previous established equation.

(f) When n = 10, 000, what is the probability that the jth observation
is in the bootstrap sample?

**Answer: **

When n = 10, 000, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/10000)^10000 = 0.632$ according to previous established equation.

(g) Create a plot that displays, for each integer value of n from 1
to 100, 000, the probability that the jth observation is in the
bootstrap sample. Comment on what you observe.

```{r, eval=T}
library(tidyverse)

n = 1:100000
data.frame(n, prob = 1 - (1 - 1/n)^n) %>%
  ggplot(aes(x = n, y = prob)) +
  geom_line(size = 1) +
  labs(x = "n", y = "Probability", title = "Probability that the jth observation is in the bootstrap sample") +
  theme_bw()
```
Since the graph seems to converge to a value around 0.6 and it cuts really close to x=0 in the plot above, lets zoom in and restrict the range of n to 100 to get a better look.

```{r, eval=T}
library(tidyverse)

n = 1:100
data.frame(n, prob = 1 - (1 - 1/n)^n) %>%
  ggplot(aes(x = n, y = prob)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 1 - (1 - 1/100000)^100000, 
             linetype = "dashed", color = "red") +
  annotate("text", x = 3, y = 0.642, label = "y = 0.632", color = "red") +
  labs(x = "n", y = "Probability", title = "Probability that the jth observation is in the bootstrap sample") +
  theme_bw()
```
As we can see from the plot, the probability that the jth observation is in the bootstrap sample converges to 0.632 as n increases.

(h) We will now investigate numerically the probability that a bootstrap
sample of size n = 100 contains the jth observation. Here
j = 4. We repeatedly create bootstrap samples, and each time
we record whether or not the fourth observation is contained in
the bootstrap sample.

```{r, eval=T}
store <- rep(NA, 10000)
for(i in 1:10000){
  store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0
}
mean(store)
```
Comment on the results obtained.

**Answer: **
The probability that a bootstrap sample of size n = 100 contains the 4th observation is 0.637 as we observed, which is very close to the value we would obtain if we are using math $1 - (1 - 1/100)^100 = 0.634$.

## ISL Exercise 5.4.9 (20pts)

We will now consider the Boston housing data set, from the ISLR2 library.
```{r, eval=T}
library(ISLR2)
data("Boston")
Boston %>% 
  print(width=10, n=inf)
```

(a) Based on this data set, provide an estimate for the population
mean of medv. Call this estimate ˆμ.

**Answer: **

```{r, eval=T}
(mu.hat = mean(Boston$medv))
```

(b) Provide an estimate of the standard error of ˆμ. Interpret this
result.
Hint: We can compute the standard error of the sample mean by
dividing the sample standard deviation by the square root of the
number of observations.

**Answer: **

```{r, eval=T}
(se.muhat = sd(Boston$medv) / sqrt(length(Boston$medv)))
```

(c) Now estimate the standard error of ˆμ using the bootstrap. How
does this compare to your answer from (b)?

**Answer: **

```{r, eval=T}
library(boot)
library(dplyr)

set.seed(1)
boot.fn <- function(data, index) return(mean(data[index]))

(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμ using the bootstrap is 0.411, which is very close to the value we obtained from (b) 0.408.

(d) Based on your bootstrap estimate from (c), provide a 95 % confidence
interval for the mean of medv. Compare it to the results
obtained using t.test(Boston$medv).
Hint: You can approximate a 95 % confidence interval using the
formula [ˆμ − 2SE(ˆμ), ˆμ + 2SE(ˆμ)].

**Answer: **

```{r, eval=T}
se.boot = sd(res$t)
round(c(mu.hat - 2*se.boot, mu.hat + 2*se.boot),4)
```

```{r, eval=T}
(t.test(Boston$medv)$conf.int)
```
The 95% confidence intervals produced with these two methods are very close, equal up to 1 decimal place.

(e) Based on this data set, provide an estimate, ˆμmed, for the median
value of medv in the population.

**Answer: **

```{r, eval=T}
(med.hat = median(Boston$medv))
```

(f) We now would like to estimate the standard error of ˆμmed. Unfortunately,
there is no simple formula for computing the standard
error of the median. Instead, estimate the standard error of the
median using the bootstrap. Comment on your findings.

**Answer: **

```{r, eval=T}
set.seed(1)
boot.fn <- function(data, index) return(median(data[index]))

(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμmed using the bootstrap is 0.3778. The standard error of the median is smaller than the standard error of the mean, which is consistent with the fact that the median is a more robust estimator.

(g) Based on this data set, provide an estimate for the tenth percentile
of medv in Boston census tracts. Call this quantity ˆμ0.1.
(You can use the quantile() function.)

**Answer: **

```{r, eval=T}
(mu0.1.hat = quantile(Boston$medv, 0.1))
```

(h) Use the bootstrap to estimate the standard error of ˆμ0.1. Comment
on your findings.

**Answer: **

```{r, eval=T}
set.seed(1)
boot.fn <- function(data, index) return(quantile(data[index], 0.1))
(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμ0.1 using the bootstrap is 0.4768, which is larger than the standard error of the median. However it is still small.

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

**Answer: **
The linear model with Gaussian errors is given by
$$
y = X\beta + \epsilon
$$

where $\epsilon \sim N(0, \sigma^2)$. The likelihood function is
$$
L(\beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta^T x_i)^2}{2\sigma^2}\right)
$$
The log-likelihood function is
$$
\ell(\beta, \sigma^2) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - \beta^T x_i)^2
$$
The least squares estimate of $\beta$ is the value that minimizes the sum of squared residuals
$$
\sum_{i=1}^N (y_i - \beta^T x_i)^2
$$
which is the same as maximizing the likelihood function.

The maximum likelihood estimate of $\beta$ is the value that maximizes $\ell(\beta, \sigma^2)$, which is the same as minimizing the sum of squared residuals. Thus, maximum likelihood and least squares are the same thing.

The AIC is defined as
$$
AIC = -2\ell(\hat{\beta}, \hat{\sigma}^2) + 2p
$$
where $p$ is the number of parameters in the model. The $C_p$ statistic is defined as
$$
C_p = \frac{1}{N} \left(\sum_{i=1}^N (y_i - \hat{\beta}^T x_i)^2 + 2p\hat{\sigma}^2\right)
$$
where $\hat{\beta}$ and $\hat{\sigma}^2$ are the least squares estimates. Since the least squares estimates are the maximum likelihood estimates, the AIC and $C_p$ are equivalent.

##TBF

## ISL Exercise 6.6.1 (10pts)

We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1
models, containing 0, 1, 2, . . . , p predictors. Explain your answers:

(a) Which of the three models with k predictors has the smallest
training RSS?

**Answer: **

The model with k predictors identified by best subset selection has the smallest training RSS. This is because best subset selection considers all possible subsets of predictors and selects the one with the smallest training RSS, thus the name best subset fitting $2^p$ possible models.

(b) Which of the three models with k predictors has the smallest
test RSS?

**Answer: **

This is uncertain and depends on the data. The model with k predictors identified by best subset selection has the smallest training RSS, but it may not have the smallest test RSS. This is because best subset selection considers all possible subsets of predictors and selects the one with the smallest training RSS, which may lead to overfitting. Forward stepwise and backward stepwise selection may have a smaller test RSS because they consider fewer models and are less likely to overfit.


(c) True or False:

i. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by forward stepwise selection.

**Answer: ** True

ii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by backward stepwise selection.

**Answer: ** True

iii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by forward stepwise selection.

**Answer: ** False

iv. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by backward stepwise selection.

**Answer: ** False

v. The predictors in the k-variable model identified by best
subset are a subset of the predictors in the (k + 1)-variable
model identified by best subset selection.

**Answer: ** False

## TBF

## ISL Exercise 6.6.3 (10pts)

Suppose we estimate the regression coefficients in a linear regression
model by minimizing

$$
\sum_{i = 1}^{n} \left( y_i - \beta_0 - \sum_{j = 1}^{p} \beta_j x_{ij} \right)^2 \quad \text{subject to} \,\, \sum_{j = 1}^{p} |\beta_j| \le s
$$

for a particular value of s. For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

(a) As we increase s from 0, the training RSS will:

**Answer: ** Steadily decrease. As s increases, the constraint becomes less restrictive, and the training RSS will decrease as the model becomes more flexible.

(b) Repeat (a) for test RSS.

**Answer: ** Decrease initially, and then eventually start increasing in a U shape. As s increases, the model becomes more flexible, and the test RSS will decrease initially. However, as s becomes very large, the model will become too flexible and the test RSS will start to increase. 

(c) Repeat (a) for variance.

**Answer: ** Steadily increase. As s increases, the model becomes more flexible, and the variance will increase. 

(d) Repeat (a) for (squared) bias. 

**Answer: ** Steadily decrease. As s increases, the model becomes more flexible, and the bias will decrease.

(e) Repeat (a) for the irreducible error. 

**Answer: ** Remain constant. The irreducible error is the error that cannot be reduced by any model, and will remain constant as s increases.

## ISL Exercise 6.6.4 (10pts) 

Suppose we estimate the regression coefficients in a linear regression
model by minimizing

$$
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$
for a particular value of !. For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

(a) As we increase $\lambda$ from 0, the training RSS will:

**Answer: ** Steadily increase. As $\lambda$ increases, the shrinkage term will increase, and the training RSS will increase as the model becomes less flexible.

(b) Repeat (a) for test RSS.

**Answer: ** Decrease initially, and then eventually start increasing in a U shape. As $\lambda$ increases, the model becomes less flexible, and the test RSS will decrease initially since the reduction in variance will outweigh the cost of shrinking. However, as $\lambda$ becomes very large, the model will become too inflexible and the test RSS will start to increase due to underfitting and increased bias outweights the decreased variance.

(c) Repeat (a) for variance.

**Answer: ** Steadily decrease. As $\lambda$ increases, the model becomes less flexible as $\beta$ shrunk towards 0, and the variance will decrease.

(d) Repeat (a) for (squared) bias.

**Answer: ** Steadily increase. As $\lambda$ increases, the model becomes less flexible as $\beta$ shrunk towards 0, and the bias will increase.

(e) Repeat (a) for the irreducible error.

**Answer: ** Remain constant. The irreducible error is the error that cannot be reduced by any model, and will remain constant as $\lambda$ increases.

## ISL Exercise 6.6.5 (10pts) 

It is well-known that ridge regression tends to give similar coefficient
values to correlated variables, whereas the lasso may give quite different
coefficient values to correlated variables. We will now explore
this property in a very simple setting.

Suppose that n = 2, p = 2, $x_{11} = x_{12}, x_{21} = x_{22}$. Furthermore,
suppose that $y_1+y_2 = 0$ and $x_{11}+x_{21} = 0$ and $x_{12}+x_{22} = 0$, so that
the estimate for the intercept in a least squares, ridge regression, or
lasso model is zero: $\hat\beta_0 = 0$.

(a) Write out the ridge regression optimization problem in this setting.

**Answer: **
$$
\begin{aligned}
X &=
\begin{bmatrix}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
\end{bmatrix}
= \begin{bmatrix}
x_{11} & x_{11} \\
x_{22} & x_{22} \\
\end{bmatrix}
\end{aligned}
$$
this infers that predictors $x_1$ and $x_2$ are perfectly correlated.

$$
\begin{aligned}
&\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2\\
&= \sum_{i=1}^{2} (y_i - \beta_0 - \sum_{j=1}^{2} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{2} \beta_j^2\\
&= (y_1 - \beta_0 - \beta_1 x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_0 - \beta_1 x_{21} - \beta_2 x_{22})^2 + \lambda (\beta_1^2 + \beta_2^2)
\end{aligned}
$$

(b) Argue that in this setting, the ridge coefficient estimates satisfy
$\hat\beta_1 = \hat\beta_2$.

**Answer: **

The ridge coefficient estimates satisfy $\hat\beta_1 = \hat\beta_2$ because the penalty term $\lambda \sum_{j=1}^{p} \beta_j^2$ will shrink the coefficients towards 0, and since the predictors $x_1$ and $x_2$ are perfectly correlated, the ridge regression will shrink the coefficients towards each other.

(c) Write out the lasso optimization problem in this setting.

**Answer: **

$$
\begin{aligned}
&\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|\\
&= \sum_{i=1}^{2} (y_i - \beta_0 - \sum_{j=1}^{2} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{2} |\beta_j|\\
&= (y_1 - \beta_0 - \beta_1 x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_0 - \beta_1 x_{21} - \beta_2 x_{22})^2 + \lambda (|\beta_1| + |\beta_2|)
\end{aligned}
$$

(d) Argue that in this setting, the lasso coefficients $\hat\beta_1$ and $\hat\beta_2$ are
not unique—in other words, there are many possible solutions
to the optimization problem in (c). Describe these solutions.

**Answer: **
The lasso coefficients $\hat\beta_1$ and $\hat\beta_2$ are not unique because the penalty term $\lambda \sum_{j=1}^{p} |\beta_j|$ will shrink the coefficients towards 0, and since the predictors $x_1$ and $x_2$ are perfectly correlated, the lasso regression will shrink the coefficients towards each other. There are many possible solutions to the optimization problem in (c) because the lasso penalty term is not differentiable at 0, and the optimization problem may have multiple solutions.

## TBF

## ISL Exercise 6.6.11 (30pts) 

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$

**Answer: **
The expected value of the training error is
$$
\begin{aligned}
\operatorname{E}[R_{\text{train}}(\hat{\beta})] &= \operatorname{E}\left[\frac{1}{N} \sum_{i=1}^N (y_i - \hat{\beta}^T x_i)^2\right] \\
&= \frac{1}{N} \sum_{i=1}^N \operatorname{E}[(y_i - \hat{\beta}^T x_i)^2] \\
&= \frac{1}{N} \sum_{i=1}^N \left(\operatorname{Var}(y_i - \hat{\beta}^T x_i) + \operatorname{E}[y_i - \hat{\beta}^T x_i]^2\right) \\
&= \frac{1}{N} \sum_{i=1}^N \left(\operatorname{Var}(y_i - \hat{\beta}^T x_i) + \left(\operatorname{E}[y_i] - \hat{\beta}^T \operatorname{E}[x_i]\right)^2\right) \\
&= \frac{1}{N} \sum_{i=1}^N \left(\operatorname{Var}(y_i - \hat{\beta}^T x_i) + \left(\hat{\beta}^T x_i - \hat{\beta}^T x_i\right)^2\right) \\
&= \frac{1}{N} \sum_{i=1}^N \left(\operatorname{Var}(y_i - \hat{\beta}^T x_i) + 0\right) \\
&= \frac{1}{N} \sum_{i=1}^N \operatorname{Var}(y_i - \hat{\beta}^T x_i) \\
&= \operatorname{Var}(y - X\hat{\beta})
\end{aligned}
$$
The expected value of the test error is
$$
\begin{aligned}
\operatorname{E}[R_{\text{test}}(\hat{\beta})] &= \operatorname{E}\left[\frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2\right] \\
&= \frac{1}{M} \sum_{i=1}^M \operatorname{E}[(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2] \\
&= \frac{1}{M} \sum_{i=1}^M \left(\operatorname{Var}(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i) + \operatorname{E}[\tilde{y}_i - \hat{\beta}^T \tilde{x}_i]^2\right) \\
&= \frac{1}{M} \sum_{i=1}^M \left(\operatorname{Var}(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i) + \left(\operatorname{E}[\tilde{y}_i] - \hat{\beta}^T \operatorname{E}[\tilde{x}_i]\right)^2\right) \\
&= \frac{1}{M} \sum_{i=1}^M \left(\operatorname{Var}(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i) + \left(\hat{\beta}^T \tilde{x}_i - \hat{\beta}^T \tilde{x}_i\right)^2\right) \\
&= \frac{1}{M} \sum_{i=1}^M \left(\operatorname{Var}(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i) + 0\right) \\
&= \frac{1}{M} \sum_{i=1}^M \operatorname{Var}(\tilde{y}_i - \hat{\beta}^T \tilde{x}_i) \\
&= \operatorname{Var}(\tilde{y} - X\hat{\beta})
\end{aligned}
$$
Since the training data and test data are drawn at random from the same population, the expected value of the training error is less than the expected value of the test error. This is because the training error is based on the same data that the model was fit to, while the test error is based on new data that the model was not fit to.