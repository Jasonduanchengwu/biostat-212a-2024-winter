---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "YOUR NAME and UID"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)

We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.

(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.

**Answer: ** 

Assume that there are n total observations, there is an equal probability of selecting any of the n observations. The probability of selecting the $j^{th}$ observation is 1/n. Thus the probability of not selecting the $j^{th}$ observation is 1 - 1/n.

(b) What is the probability that the second bootstrap observation
is not the jth observation from the original sample?

**Answer: **

Same as in part (a), the probability of not selecting the $j^{th}$ observation is 1 - 1/n.

(c) Argue that the probability that the jth observation is not in the
bootstrap sample is (1 − 1/n)n.

**Answer: **

since we sample n times in total and each sampling is independent, the probability of not selecting the $j^{th}$ observation is $(1 - 1/n)^n$.

(d) When n = 5, what is the probability that the jth observation is
in the bootstrap sample?

**Answer: **

When n = 5, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/5)^5 = 0.672$ according to previous established equation.

(e) When n = 100, what is the probability that the jth observation
is in the bootstrap sample?

**Answer: **

When n = 100, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/100)^100 = 0.634$ according to previous established equation.

(f) When n = 10, 000, what is the probability that the jth observation
is in the bootstrap sample?

**Answer: **

When n = 10, 000, the probability that the jth observation is in the bootstrap sample is $1 - (1 - 1/10000)^10000 = 0.632$ according to previous established equation.

(g) Create a plot that displays, for each integer value of n from 1
to 100, 000, the probability that the jth observation is in the
bootstrap sample. Comment on what you observe.

```{r, eval=T}
library(tidyverse)

n = 1:100000
data.frame(n, prob = 1 - (1 - 1/n)^n) %>%
  ggplot(aes(x = n, y = prob)) +
  geom_line(size = 1) +
  labs(x = "n", y = "Probability", title = "Probability that the jth observation is in the bootstrap sample") +
  theme_bw()
```
Since the graph seems to converge to a value around 0.6 and it cuts really close to x=0 in the plot above, lets zoom in and restrict the range of n to 100 to get a better look.

```{r, eval=T}
library(tidyverse)

n = 1:100
data.frame(n, prob = 1 - (1 - 1/n)^n) %>%
  ggplot(aes(x = n, y = prob)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 1 - (1 - 1/100000)^100000, 
             linetype = "dashed", color = "red") +
  annotate("text", x = 3, y = 0.642, label = "y = 0.632", color = "red") +
  labs(x = "n", y = "Probability", title = "Probability that the jth observation is in the bootstrap sample") +
  theme_bw()
```
As we can see from the plot, the probability that the jth observation is in the bootstrap sample converges to 0.632 as n increases.

(h) We will now investigate numerically the probability that a bootstrap
sample of size n = 100 contains the jth observation. Here
j = 4. We repeatedly create bootstrap samples, and each time
we record whether or not the fourth observation is contained in
the bootstrap sample.

```{r, eval=T}
store <- rep(NA, 10000)
for(i in 1:10000){
  store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0
}
mean(store)
```
Comment on the results obtained.

**Answer: **
The probability that a bootstrap sample of size n = 100 contains the 4th observation is 0.637 as we observed, which is very close to the value we would obtain if we are using math $1 - (1 - 1/100)^100 = 0.634$.

## ISL Exercise 5.4.9 (20pts)

We will now consider the Boston housing data set, from the ISLR2 library.
```{r, eval=T}
library(ISLR2)
data("Boston")
Boston %>% 
  print(width=10, n=inf)
```

(a) Based on this data set, provide an estimate for the population
mean of medv. Call this estimate ˆμ.

**Answer: **

```{r, eval=T}
(mu.hat = mean(Boston$medv))
```

(b) Provide an estimate of the standard error of ˆμ. Interpret this
result.
Hint: We can compute the standard error of the sample mean by
dividing the sample standard deviation by the square root of the
number of observations.

**Answer: **

```{r, eval=T}
(se.muhat = sd(Boston$medv) / sqrt(length(Boston$medv)))
```

(c) Now estimate the standard error of ˆμ using the bootstrap. How
does this compare to your answer from (b)?

**Answer: **

```{r, eval=T}
library(boot)
library(dplyr)

set.seed(1)
boot.fn <- function(data, index) return(mean(data[index]))

(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμ using the bootstrap is 0.411, which is very close to the value we obtained from (b) 0.408.

(d) Based on your bootstrap estimate from (c), provide a 95 % confidence
interval for the mean of medv. Compare it to the results
obtained using t.test(Boston$medv).
Hint: You can approximate a 95 % confidence interval using the
formula [ˆμ − 2SE(ˆμ), ˆμ + 2SE(ˆμ)].

**Answer: **

```{r, eval=T}
se.boot = sd(res$t)
round(c(mu.hat - 2*se.boot, mu.hat + 2*se.boot),4)
```

```{r, eval=T}
(t.test(Boston$medv)$conf.int)
```
The 95% confidence intervals produced with these two methods are very close, equal up to 1 decimal place.

(e) Based on this data set, provide an estimate, ˆμmed, for the median
value of medv in the population.

**Answer: **

```{r, eval=T}
(med.hat = median(Boston$medv))
```

(f) We now would like to estimate the standard error of ˆμmed. Unfortunately,
there is no simple formula for computing the standard
error of the median. Instead, estimate the standard error of the
median using the bootstrap. Comment on your findings.

**Answer: **

```{r, eval=T}
set.seed(1)
boot.fn <- function(data, index) return(median(data[index]))

(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμmed using the bootstrap is 0.3778. The standard error of the median is smaller than the standard error of the mean, which is consistent with the fact that the median is a more robust estimator.

(g) Based on this data set, provide an estimate for the tenth percentile
of medv in Boston census tracts. Call this quantity ˆμ0.1.
(You can use the quantile() function.)

**Answer: **

```{r, eval=T}
(mu0.1.hat = quantile(Boston$medv, 0.1))
```

(h) Use the bootstrap to estimate the standard error of ˆμ0.1. Comment
on your findings.

**Answer: **

```{r, eval=T}
set.seed(1)
boot.fn <- function(data, index) return(quantile(data[index], 0.1))
(res = boot(Boston$medv, boot.fn, 1000))
```
The standard error of ˆμ0.1 using the bootstrap is 0.4768, which is larger than the standard error of the median. However it is still small.

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

**Answer: **
The linear model with Gaussian errors is given by
$$
y = X\beta + \epsilon
$$

where $\epsilon \sim N(0, \sigma^2)$. The likelihood function is
$$
L(\beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta^T x_i)^2}{2\sigma^2}\right)
$$
The log-likelihood function is
$$
\ell(\beta, \sigma^2) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - \beta^T x_i)^2
$$
The least squares estimate of $\beta$ is the value that minimizes the sum of squared residuals
$$
\sum_{i=1}^N (y_i - \beta^T x_i)^2
$$
which is the same as maximizing the likelihood function.

The maximum likelihood estimate of $\beta$ is the value that maximizes $\ell(\beta, \sigma^2)$, which is the same as minimizing the sum of squared residuals. Thus, maximum likelihood and least squares are the same thing.

The AIC is defined as
$$
AIC = -2\ell(\hat{\beta}, \hat{\sigma}^2) + 2p
$$
where $p$ is the number of parameters in the model. The $C_p$ statistic is defined as
$$
C_p = \frac{1}{N} \left(\sum_{i=1}^N (y_i - \hat{\beta}^T x_i)^2 + 2p\hat{\sigma}^2\right)
$$
where $\hat{\beta}$ and $\hat{\sigma}^2$ are the least squares estimates. Since the least squares estimates are the maximum likelihood estimates, the AIC and $C_p$ are equivalent.

##TBF

## ISL Exercise 6.6.1 (10pts)

## ISL Exercise 6.6.3 (10pts)

## ISL Exercise 6.6.4 (10pts)

## ISL Exercise 6.6.5 (10pts)

## ISL Exercise 6.6.11 (30pts)

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$